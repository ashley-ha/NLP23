{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZ2oE4GmdfF"
      },
      "source": [
        "# Homework 3: Language Models, Contextual Embedding and BERT\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will explore BERT and measure perplexity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0EWMnOmgWw"
      },
      "source": [
        "##Set Up\n",
        "\n",
        "If you're opening this Notebook on colab, you will probably need to install Transformers. Make sure your version of Transformers is at least 4.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svbBi3YGmMJX",
        "outputId": "9a4aac6e-72a8-4cf8-dc6f-53bac6d56420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhnQ3PoVnH7D",
        "outputId": "ebc4a9df-2404-4728-9d95-74e671c8b297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.26.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEDSQsLnbLm"
      },
      "source": [
        "IMPORTANT: For this assignment, GPU is not necessary. The following code block should show \"Running on cpu\". \n",
        "Go to Runtime > Change runtime type > Hardware accelerator > None if otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7osYx6Hm0vh",
        "outputId": "5c4259b3-f277-41c1-feb5-b7da3069c0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4XlcOvIHgl"
      },
      "source": [
        "# Masking\n",
        "\n",
        "One of the core ideas to wrap your head around with transformer-based language models (and PyTorch) is the concept of *masking*---preventing a model from seeing specific tokens in the input during training.\n",
        "\n",
        "* BERT training relies on the concept of *masked language modeling*: masking a random set of input tokens in a sequence and attempting to predict them.  Remember that BERT is *bidirectional*, so that it can use all of the other non-masked tokens in a sentence to make that prediction.\n",
        "\n",
        "* The GPT class of models acts as a traditional left-to-right language model (sometimes called a \"causal\" LM) .  This family also uses self-attention based transformers---but, when making a prediction for the word $w_i$ at position $i$, it can only use information about words $w_1, \\ldots, w_{i-1}$ to do so.  All of the other tokens following position $i-1$ must be *masked* (hidden from view).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0LW-vKZnIH"
      },
      "source": [
        "Think about a mask as a matrix that's applied to every input $w$ when generating an output $o$ that determines whether an given $o_i$ is allowed to access each token in $w$.  For example, when passing a three-word input sequence through a transformer (to yield a three-word output sequence), a mask is a $3 \\times 3$ matrix where the cells are essentially  answering the following questions:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "o_1 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_2 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_3 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "In the masks we will consider below, 1 denotes that a position should be hidden; 0 denotes that it should be visible. Consider this mask:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "And consider this sequence:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "\\textrm{John} & \\textrm{likes}  & \\textrm{dogs}  \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "When applying this mask to that sequence, we're saying that when we're generating the output for $o_1$ (*John*), we can only consider $w_1$ as an input (*John*).  Likewise, when we generate the output for $o_2$ (*likes*), we can only consider $w_2$ as an input (*likes*), and so on.  (This is a terrible mask!  But illustrates what function a mask performs.)\n",
        "\n",
        "The following code illustrates how this works for that particular mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ea2E6zlMZkzO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def visualize_masking(sequences, mask):\n",
        "  print(mask)\n",
        "  for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "      visible=[]\n",
        "      for j in range(len(sequence)):\n",
        "        if mask[i][j]==0:\n",
        "          visible.append(sequence[j])\n",
        "      print(\"for word %s, the following tokens are visible: %s\" % (sequence[i], visible))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xROu5KmvKMua",
        "outputId": "8fc63b98-8331-4012-8171-bfe233aed37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
            "for word This, the following tokens are visible: ['This']\n",
            "for word is, the following tokens are visible: ['is']\n",
            "for word a, the following tokens are visible: ['a']\n",
            "for word sentence, the following tokens are visible: ['sentence']\n",
            "for word that, the following tokens are visible: ['that']\n",
            "for word has, the following tokens are visible: ['has']\n",
            "for word exactly, the following tokens are visible: ['exactly']\n",
            "for word ten, the following tokens are visible: ['ten']\n",
            "for word tokens, the following tokens are visible: ['tokens']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n",
            "for word Here's, the following tokens are visible: [\"Here's\"]\n",
            "for word another, the following tokens are visible: ['another']\n",
            "for word sequence, the following tokens are visible: ['sequence']\n",
            "for word with, the following tokens are visible: ['with']\n",
            "for word 10, the following tokens are visible: ['10']\n",
            "for word words, the following tokens are visible: ['words']\n",
            "for word like, the following tokens are visible: ['like']\n",
            "for word the, the following tokens are visible: ['the']\n",
            "for word last, the following tokens are visible: ['last']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\t\n",
        "\n",
        "seq_length=len(sequences[0])\n",
        "\n",
        "test_mask=np.ones((seq_length,seq_length))\n",
        "for i in range(seq_length):\n",
        "  test_mask[i,i]=0\n",
        "\n",
        "visualize_masking(sequences, test_mask) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv3h625eMo3"
      },
      "source": [
        "##Q1.  \n",
        "As we discussed in class, BERT masks a random set of words in the input and attempts to reconstruct those words as output.  Create a mask that randomly masks token positions 2 and 7 (for an input sequence length of 10 tokens, with 0 being the position of the first token).  For an input sequence of 10 tokens, you should generate output representations for all 10 tokens (i.e., $[o_1, \\ldots, o_{10}]$ in the notation above, but each representation must ignore the same 2 input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "_31mkhIke-YI"
      },
      "outputs": [],
      "source": [
        "def create_bert_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  for i in range(seq_length):\n",
        "    mask[i][2] = 1\n",
        "    mask[i][7] = 1\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_bert_mask(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6OnwUY5R_Ya",
        "outputId": "b78b964f-3bfc-4bb1-fcc1-3b0015c63189"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKjasreUfLww"
      },
      "source": [
        "##Q2\n",
        "A left-to-right language model (such as GPT) can only use information from input words $[w_1, \\ldots, w_{i}]$ when generating the representation for output $o_i$.  Encode this as a mask as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "svKuQyQff4fK"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  for i in range(seq_length):\n",
        "    for j in range(i+1, seq_length):\n",
        "      mask[i][j] = 1\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_causal_mask(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4ukzkVITWeJ",
        "outputId": "834d8707-162a-436d-c286-44857123fa84"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 1., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRkI2DZnwvB"
      },
      "source": [
        "Now let's go ahead and embed these masks within a model.  First, we'll load some textual data (from Austen's *Pride and Prejudice*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "cgTwRcqDn6ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f4b171-6321-4dc5-bf2e-31d059fcea03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 00:18:02--  https://www.gutenberg.org/files/1342/1342-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772145 (754K) [text/plain]\n",
            "Saving to: ‘1342-0.txt.3’\n",
            "\n",
            "\r1342-0.txt.3          0%[                    ]       0  --.-KB/s               \r1342-0.txt.3        100%[===================>] 754.05K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-02-22 00:18:02 (7.89 MB/s) - ‘1342-0.txt.3’ saved [772145/772145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "7v2EJeEBobD8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "u8hjeySdojyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0112587a-d017-4654-834b-739250c78402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jre3sSWgvnwi"
      },
      "source": [
        "Let's read in the data and tokenize it; for this homework, we'll only work with the first 10,000 tokens of that book; we'll keep only the most frequent 1,000 word types (all other tokens will be mapped to an [UNK] token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "7kzw9IcFoDZq"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "  with open(filename) as file:\n",
        "    data=file.read().lower()\n",
        "    first10K=' '.join(data.split(\" \")[:10000])\n",
        "    toks=nltk.word_tokenize(first10K)[:10000]\n",
        "    vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "    counts=Counter()\n",
        "    for tok in toks:\n",
        "      counts[tok]+=1\n",
        "    for v, _ in counts.most_common(1000):\n",
        "      vocab[v]=len(vocab)\n",
        "    tokids=[]\n",
        "    for tok in toks:\n",
        "      tokid=1\n",
        "      if tok in vocab:\n",
        "        tokid=vocab[tok]\n",
        "      \n",
        "      tokids.append(tokid)\n",
        "\n",
        "    return tokids, vocab  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_XHBZPv4kD"
      },
      "source": [
        "Now let's specify our model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "p74L0s1DsKa2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MaskedLM(nn.Module):\n",
        "    def __init__(self, vocab, mask, d_model=512):       \n",
        "        super().__init__()\n",
        "        self.vocab=vocab\n",
        "        self.mask=mask\n",
        "        vocab_size=len(vocab)\n",
        "        self.embeddings=nn.Embedding(1002,512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear=torch.nn.Linear(d_model, vocab_size)\n",
        "        self.rev_vocab={vocab[k]:k for k in vocab}\n",
        "\n",
        "    def forward(self, input): \n",
        "        # first we pass the input word IDS through an embedding layer to get embeddings for them\n",
        "        input=self.embeddings(input)\n",
        "        # then we pass those embeddings through a transformer to get contextual representations, masking the input where appropriate\n",
        "        out = self.transformer_encoder.forward(input, mask=self.mask)        \n",
        "        # finally we pass those embeddings through a linear layer to transform it into the output space (the size of our vocabulary)\n",
        "        h=self.linear(out)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "njdPnGVQsQOl"
      },
      "outputs": [],
      "source": [
        "def get_batches(xs, ys, batch_size=32):\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    for i in range(0, len(xs), batch_size):\n",
        "        batch_x.append(torch.LongTensor(xs[i:i+batch_size]).to(device))\n",
        "        batch_y.append(torch.LongTensor(ys[i:i+batch_size]).to(device))\n",
        "    return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "xm6tLzHmtAfN"
      },
      "outputs": [],
      "source": [
        "tokids, vocab=read_data(\"1342-0.txt\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "9822mHI3sR1Q"
      },
      "outputs": [],
      "source": [
        "def train(mask, data_function, tokids, vocab):\n",
        "\n",
        "    mask=torch.BoolTensor(mask).to(device)\n",
        "\n",
        "    num_labels=len(vocab)\n",
        "    model=MaskedLM(vocab, mask).to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "    losses=[]\n",
        "\n",
        "    xs, ys=data_function(tokids)\n",
        "\n",
        "    batch_x, batch_y=get_batches(xs, ys)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "        \n",
        "        for x, y in list(zip(batch_x, batch_y)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred=model.forward(x)\n",
        "            loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            print(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CXKNwLBwBG6"
      },
      "source": [
        "Our model and training process are now all defined; all that remains is to pass our inputs and outputs through it to train.  Your job here is to create the correct inputs (x) and outputs (y) to train a left-to-right (causal) language model.\n",
        "\n",
        "##Q3\n",
        "Write a function that takes in a sequence of token ids $[w_1, \\ldots, w_n]$ and segments it into 8-token chunks -- e.g., $x_1=[w_1, \\ldots, w_8]$, $x_2=[w_9, \\ldots, w_{16}]$, etc.  For each $x_i$, also create its corresponding $y_i$.  Given this language modeling specification, each $y_i$ should also contain 8 values (for each token in $x_i$).  Keep in mind this is a left-to-right causal language model; your job is to figure out the values of y that respects this design.  At token position $i$, when a model has access to $[w_1, \\ldots, w_i]$, which is the true $y_i$ for that position? Each element in $y$ should be a word ID (i.e., an integer)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_causal_xy(data, max_len=8):\n",
        "    xs=[]\n",
        "    ys=[]\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        x = data[i:i+max_len]\n",
        "        if len(x) == max_len:\n",
        "          xs.append(x)\n",
        "          y = data[i+1:(i+1) + max_len]\n",
        "          if len(y) < max_len:\n",
        "            y += [0] * (max_len - len(y)) \n",
        "          ys.append(y)\n",
        "    # END SOLUTION\n",
        "    return xs, ys"
      ],
      "metadata": {
        "id": "1rqdPxbQosRZ"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_causal_xy(tokids, max_len = 8)"
      ],
      "metadata": {
        "id": "Rw8MkSKLaCVc"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "AO4WOusZsWsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299a2c1c-7ad9-4b29-a8e5-83cd3a84ca59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7.1567, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4181, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1173, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4863, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3432, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6563, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3107, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4411, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7007, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7964, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8205, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1478, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4812, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0435, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3599, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4516, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.4014, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5037, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4752, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6276, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9890, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1741, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2791, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2001, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6469, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7658, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1594, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8470, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5243, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1788, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2345, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3953, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6554, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1821, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6011, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4844, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6040, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3246, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4494, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1718, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4388, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8444, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6749, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3624, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3872, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3911, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6690, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8994, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9492, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3066, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0473, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6421, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5760, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2390, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4238, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8580, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6356, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.3511, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5029, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3573, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3489, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0549, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0138, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4851, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2804, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2923, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8272, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6132, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.9360, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1507, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8516, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6205, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7231, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7120, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0839, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8862, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.2846, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.3999, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5777, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7798, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6375, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4075, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7992, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4015, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6350, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5869, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0939, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9642, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1803, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8482, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2918, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4701, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8127, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1358, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4153, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7414, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0824, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8804, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4191, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3122, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7132, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2335, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9261, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9227, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1993, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1512, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2871, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7327, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0633, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7987, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9089, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0747, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9356, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0985, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3403, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2218, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0921, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1280, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6376, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7508, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4227, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6576, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9776, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7148, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6107, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7751, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0474, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0353, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9172, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2343, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6831, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4577, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3336, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5791, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7866, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9669, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0903, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8518, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2563, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9448, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1988, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6632, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5554, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8175, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1456, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7322, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1930, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6331, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1721, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6759, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9483, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0605, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4272, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3620, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3306, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5919, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1178, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8018, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1422, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8166, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3648, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5566, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4095, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7592, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3458, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0899, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3277, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9930, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7650, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8251, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3606, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3554, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0563, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8433, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2570, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2795, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9393, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1239, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6858, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1160, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7615, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6604, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0406, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7270, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4741, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1040, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5594, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7009, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1735, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3722, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5303, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5172, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "seq_length=8\n",
        "\n",
        "train(create_causal_mask(seq_length=seq_length), get_causal_xy, tokids, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2P8GU1VsKLn"
      },
      "source": [
        "##Q4 (Write-up)  \n",
        "In this model, as implemented, does the following equivalence hold?\n",
        "\n",
        "$$\n",
        "P(y_4 \\mid w_1 = \\textrm{go}, w_2=\\textrm{ahead}, w_3=\\textrm{make}, w_4=\\textrm{my})= P(y_4 \\mid w_1 = \\textrm{ahead}, w_2=\\textrm{my}, w_3=\\textrm{make}, w_4=\\textrm{go})\n",
        "$$\n",
        "\n",
        "Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZpVrqchSAI"
      },
      "source": [
        "# Perplexity\n",
        "To evaluate how good our language model is, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$\n",
        "\n",
        "Here we want to calculate the perplexity of [pretrained BERT model](https://huggingface.co/bert-base-uncased) on text from different sources. When calculating perplexity with BERT, we'll use a related measure of pseudo-perplexity, which allow us to condition on the bidirectional context (and not just the left context, as in standard perplexity):\n",
        "\n",
        "$$PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i} \\mid w_{1}\\dots w_{i - 1}, w_{i+1}, \\ldots, w_n)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2MXSfzlsHp_"
      },
      "source": [
        "First, let's instantiate a BERT model, along with its WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "de281ac602534a7d8d8e78ebe4c6a5e2",
            "7c9d6528cb304849b1fe7fba339a46f4",
            "3972be8a14d64e7bbff0c5b9595cb952",
            "53c807c61e5d4e40bfe7074ece30b075",
            "46a91a8363ec4368b2191b2869f991b9",
            "58daf567c2e148c7ae514ed58615adff",
            "01a29770035642cf9c9d3ddeb3220f32",
            "6b4f326ba37f4a47a121bcfb4dd86bb8",
            "8131b083583d44d09f3d37001e8e50ec",
            "27e8dce90c1e4b088fb82bea510ca30b",
            "60fcb3ba73454e21b1b09dd5e5054bdc",
            "1c21aa4fb56948b98ac5105ca484079e",
            "c11834c0baed4e5790f08877d93e17d1",
            "f049886c3e2247659b44c723dd08e636",
            "9bdc693393ec44ca93bfb40e3b9bcb35",
            "6fb7dccba59d41d68b31aea69119b1b9",
            "f638136b7ff946c49c060095acadb8ea",
            "d74b3a6731fc43e3962d103b15d31adb",
            "bb77a3bcbdce4eed945d4da037f9e3ad",
            "4f54b637ab8a40e2a0d60660e68c9e18",
            "e95dd77dcfdf4ccd95dc0479d2cc687d",
            "90d62e7c1b8c418d823bb2646768d1c1",
            "81f030b4c22242018470f71a09734a9d",
            "0e04d66555054d0fa0f102577855b06b",
            "2b2a2e1827264ce5bd3e7d8a1e545d29",
            "1633581bcd594a65be41bc08d1500f47",
            "0b080e5c16f64116861265c3a0f1407c",
            "80dd42969963458bbfe608649994bd17",
            "f61f7c3891f445d49e15f948886552b5",
            "e4a61d2eb71646dc8156da81c156d44f",
            "e1fa545a92214bd99397551a285a55ee",
            "4f1bfce518d34e71b0592ece1f4c01a3",
            "8606f9022bc249438c636d85aeca946f",
            "5cb0c3cd143343c4a0cdca0a52bcebd5",
            "374b0557ce2544109f837cfbd677bd87",
            "e67fd60d2b214b2088bc40629a17fef8",
            "654399629b4a42d9bbeea3bdf55f524b",
            "f482e067586f49cc96827b2d2a5e3004",
            "492871578c3c4e9e909e2fdbeee6cb9f",
            "7f37dd70377141ee8d6786543f4facab",
            "13e10cf5a9b84a619e565fc730447405",
            "eeb10c1c5d3b49c8ad1c450f14aacce4",
            "8c7e65052db74752a39c50dbc7d9ec09",
            "8f81a5e495754a03ba3d2d4b99e3752b",
            "99c98c6daef44a769fadfd31efe60212",
            "4c6135490cbb4a5cb5bbe56d2210a759",
            "3b8d547835ac4a8780a78b39aea1ee89",
            "e642ea1dfae24d80a4ece7c4b98d0701",
            "9f932a0e10ae4f0baecd1f95c04d5988",
            "fb33d261b427495bbcae82fed3c5506b",
            "4e464fce8e6948b7968a63c743c4e185",
            "27d32634548148baba5b6bf91ba04217",
            "062298e60b3b43ef8edd2e978a03d7e1",
            "68c5f0ba9f994e15817feeee55eb7e69",
            "44fb6c7c03f64e3cac23d4668adc21f1"
          ]
        },
        "id": "XxyJaS1-cP3R",
        "outputId": "9f68e1b2-6af8-4714-bf7e-4472269d07a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de281ac602534a7d8d8e78ebe4c6a5e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c21aa4fb56948b98ac5105ca484079e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81f030b4c22242018470f71a09734a9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cb0c3cd143343c4a0cdca0a52bcebd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99c98c6daef44a769fadfd31efe60212"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLAWTU8sTqf"
      },
      "source": [
        "Let's see how the BERT tokenizer tokenizes a sentence into a sequence of WordPiece ids.  Note how BERT tokenization automatically wraps an input sentences with [CLS] and [SEP] tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlOsB0NshJm",
        "outputId": "a98ca4a7-d9ec-4466-d2bc-bb7737b761fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]])\n",
            "['[CLS]', 'a', 'dog', 'landed', 'on', 'mars', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"A dog landed on Mars\"\n",
        "tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tensor_input)\n",
        "tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "print(tensor_input_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(tensor_input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGu38_DB1vsB"
      },
      "source": [
        "Now let's see how we can calculate output probabilities using this model.  The output of each token position $i$ gives us $P(w_i \\mid w_1, \\ldots, w_n)$---the probability of the word at that position over our vocabulary, given *all* of the words in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuNw52m15OU",
        "outputId": "aedc0904-6d8a-47cf-b2e7-46e94c5e77ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 30522])\n",
            "[CLS]\t101\t0.00000\n",
            "a\t1037\t0.99281\n",
            "dog\t3899\t0.99052\n",
            "landed\t5565\t0.99809\n",
            "on\t2006\t0.99874\n",
            "mars\t7733\t0.00133\n",
            "[SEP]\t102\t0.00000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output = model(tensor_input_ids)\n",
        "  logits = output.logits\n",
        "  # logits here are the unnormalized scores, so let's pass them through the softmax \n",
        "  # to get a probability distribution\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  # for one input sequence, the shape of the resulting distribution is: \n",
        "  # 1 x [length of input, in WordPiece tokens] x (the size of the BERT vocabulary)\n",
        "  print(softmax.shape) # [1, 7, 30522]\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  # Let's print the probability of the true inputs\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  for i in range(len(input_ints)):\n",
        "    prob=softmax[0][i][input_ints[i]].numpy()\n",
        "    print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F90uSrL_1vot"
      },
      "source": [
        "Note that $w_i$ is in the range $[w_1, \\ldots, w_n]$ -- clearly the probability of a word is going to be high when we can observe it in the input! Let's do some masking to calculate $P(w_i \\mid w_1, \\ldots w_{i-1}, w_{i+1}, w_n)$.  Now annoyingly, BERT's `attention_mask` function only works for padding tokens; to mask input tokens, we need to intervene in the input and replace a WordPiece token that we're predicting with a special [MASK] token (BERT tokenizer word id `103`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77EdDCtk3MSa",
        "outputId": "ca27e8f9-1127-487b-ec85-4c5bb9b27ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The second word here now is [MASK] token ID '103':  tensor([[ 101,  103, 3899, 5565, 2006, 7733,  102]])\n",
            "a\t1037\t0.13965\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "with torch.no_grad():\n",
        "  # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "  masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "  # we'll mask the second word\n",
        "  masked_input_ids[0][1]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "  print(\"The second word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "\n",
        "  # now let's run that through BERT in the same way we did before\n",
        "  output = model(masked_input_ids)\n",
        "  logits = output.logits\n",
        "\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  i=1\n",
        "  prob=softmax[0][i][input_ints[i]].numpy()\n",
        "  print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdj7onm1vln"
      },
      "source": [
        "You can see the probability of \"a\" as the second token has gone down to 0.13965 when we mask it.  This is the $P(w_1 =\\textrm{a} \\mid w_0, w_2, \\ldots, w_n)$.  At this point you should have everything you need to calculate the BERT pseudo-perplexity of an input sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_eHLic_OfM"
      },
      "source": [
        "##Q5\n",
        "Implement the pseudo-perplexity measure described above, calculating the perplexity for a given model, tokenizer, and sentence. \n",
        "\n",
        "The function calculates the average probability of each token in the sentence given all the other tokens. We need to predict the probability of each word in a sentence by masking the one word to predict. Note that you should not include the probabilities of the [CLS] and [SEP] tokens in your perplexity equation -- those tokens are not part of the original test sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "id": "GDZ9Qe8L-uUt"
      },
      "outputs": [],
      "source": [
        "# This function calculates the perplexity of a language model, given a sentence and its corresponding tokenizer\n",
        "\n",
        "# Inputs:\n",
        "# model: language model being used to calculate the perplexity\n",
        "# tokenizer: tokenizer that is used to preprocess the input sentence\n",
        "# sentence: input sentence string for which perplexity is to be calculated\n",
        "\n",
        "# Outputs:\n",
        "# returns perplexity of the input sentence\n",
        "import copy\n",
        "import math\n",
        "def perplexity(model, tokenizer, sentence):\n",
        "\n",
        "    # hints: you'll need to:\n",
        "    # encode the input sentence using the tokenizer\n",
        "    # for each WordPiece token in the sentence (except [CLS] and [SEP]), mask that single token and \n",
        "    # calculate the probability of that true word at the masked position\n",
        "    # don't calculate perplexity for the [CLS] and [SEP] tokens (which are not part of the original test sentence).\n",
        "    \n",
        "    perplexity=None\n",
        "    # BEGIN SOLUTION \n",
        "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0) #Returns a new tensor with a dimension of size one inserted at the specified position\n",
        "    with torch.no_grad():\n",
        "      word = 0\n",
        "      for i in range(1, len(input_ids[0]) - 1):\n",
        "        # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "        masked_input_ids = copy.deepcopy(input_ids)\n",
        "        # we'll mask the second word\n",
        "        masked_input_ids[0][i] = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "        # now let's run that through BERT in the same way we did before\n",
        "        output = model(masked_input_ids)\n",
        "        logits = output.logits\n",
        "\n",
        "        softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "        input_ints = input_ids.numpy()[0]\n",
        "\n",
        "        wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "        prob = softmax[0][i][input_ints[i]].numpy()\n",
        "        word += 1\n",
        "        print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))\n",
        "        prob = math.log(prob)\n",
        "    perplexity = math.exp(-prob / word)\n",
        "    # END SOLUTION\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "id": "tN7HVo_oIWXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc262488-9d4b-4289-e98c-56b1059b3f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "london\t2414\t0.64463\n",
            "is\t2003\t0.97455\n",
            "the\t1996\t0.99652\n",
            "capital\t3007\t0.97712\n",
            "of\t1997\t0.99983\n",
            "the\t1996\t0.99996\n",
            "united\t2142\t0.99991\n",
            "kingdom\t2983\t0.99916\n",
            ".\t1012\t0.97146\n",
            "1.0032227125935136\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(sentence='London is the capital of the United Kingdom.', model=model, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsHLgqH0-6_t"
      },
      "source": [
        "# No credit.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5eR8EF12v1s"
      },
      "source": [
        "We provide [texts](https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt) from 4 different sources ([Wikipedia](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots), [Yelp](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset), [Fiction](https://github.com/dbamman/litbank), [Twitter](https://github.com/dbamman/anlp21/blob/main/data/potus_tweets.json)) collected from open-source datasets. Each category has 125 entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnkdWOu1BGjP",
        "outputId": "f05e4bc9-d11d-4ebf-f909-be6217559be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 01:44:20--  https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61117 (60K) [text/plain]\n",
            "Saving to: ‘text_from_different_sources.txt.1’\n",
            "\n",
            "text_from_different 100%[===================>]  59.68K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-22 01:44:20 (421 KB/s) - ‘text_from_different_sources.txt.1’ saved [61117/61117]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpCpGQpG4OJZ",
        "outputId": "895fa3cb-72e5-494a-c3b2-88476212a567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia 125\n",
            "Yelp 125\n",
            "Fiction 125\n",
            "Twitter 125\n"
          ]
        }
      ],
      "source": [
        "text_by_genre={}\n",
        "with open('text_from_different_sources.txt') as file:\n",
        "  file.readline()\n",
        "  for line in file:\n",
        "    cols=line.rstrip().split(\"\\t\")\n",
        "    genre=cols[0]\n",
        "    text=cols[1]\n",
        "\n",
        "    if genre not in text_by_genre:\n",
        "      text_by_genre[genre]=[]\n",
        "    text_by_genre[genre].append(text)\n",
        "\n",
        "for genre in text_by_genre:\n",
        "  print(genre, len(text_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtE7Umzy9xRj"
      },
      "source": [
        "Calculate perplexity on each genre over all of the words present within it; each line contains exactly one sentence for each genre.\n",
        "\n",
        "The output perplexity_by_genre = {} is a dictionary mapping genre to a list of perplexities for each sentence in that genre. For computational purpose, we only take the first 25 sentences as an example (still this can take up to 10 minutes to run), feel free to change 25 to smaller numbers.\n",
        "\n",
        "e.g. perplexity_by_genre['Wikipedia'] should be a list of 25 perplexities (one for each Wikipedia row in the input file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "09NqLCO0523E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity_by_genre(text_by_genre):\n",
        "  perplexity_by_genre = {}\n",
        "  for genre in text_by_genre:\n",
        "    perplexity_by_genre[genre] = []\n",
        "    for text in text_by_genre[genre][:25]: # change 25 to smaller numbers if necessary\n",
        "      p = perplexity(sentence=text, model=model, tokenizer=tokenizer)\n",
        "      perplexity_by_genre[genre].append(p)\n",
        "  return perplexity_by_genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "7sXsSGbd730C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9f4902-7e3a-4965-c657-e1b782227218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\t1999\t0.98727\n",
            "the\t1996\t0.94360\n",
            "film\t2143\t0.49166\n",
            ",\t1010\t0.99936\n",
            "lillian\t19344\t0.00401\n",
            "travers\t29053\t0.00033\n",
            ",\t1010\t0.99935\n",
            "a\t1037\t0.99846\n",
            "wealthy\t7272\t0.12362\n",
            "northern\t2642\t0.00025\n",
            "woman\t2450\t0.40881\n",
            "about\t2055\t0.39768\n",
            "to\t2000\t1.00000\n",
            "be\t2022\t0.33734\n",
            "married\t2496\t0.44140\n",
            ",\t1010\t0.99947\n",
            "takes\t3138\t0.00526\n",
            "a\t1037\t0.97636\n",
            "magical\t8687\t0.00449\n",
            "seed\t6534\t0.00017\n",
            "which\t2029\t0.10333\n",
            "transforms\t21743\t0.55636\n",
            "its\t2049\t0.28939\n",
            "user\t5310\t0.03868\n",
            "into\t2046\t0.92361\n",
            "the\t1996\t0.92259\n",
            "opposite\t4500\t0.26866\n",
            "gender\t5907\t0.15277\n",
            ".\t1012\t0.99452\n",
            "mabel\t19486\t0.00089\n",
            "and\t1998\t0.99870\n",
            "her\t2014\t0.77075\n",
            "beau\t17935\t0.00010\n",
            "go\t2175\t0.90584\n",
            "to\t2000\t0.96152\n",
            "an\t2019\t0.84016\n",
            "auto\t8285\t0.48464\n",
            "race\t2679\t0.07169\n",
            "and\t1998\t0.99434\n",
            "are\t2024\t0.98395\n",
            "joined\t2587\t0.09151\n",
            "by\t2011\t0.99930\n",
            "charlie\t4918\t0.01569\n",
            "and\t1998\t0.86250\n",
            "his\t2010\t0.43912\n",
            "friend\t2767\t0.00885\n",
            ".\t1012\t0.99020\n",
            "in\t1999\t0.97089\n",
            "one\t2028\t0.97236\n",
            "of\t1997\t0.99976\n",
            "chaplin\t23331\t0.09606\n",
            "'\t1005\t0.99980\n",
            "s\t1055\t0.99999\n",
            "\"\t1000\t0.98987\n",
            "park\t2380\t0.00016\n",
            "comedies\t22092\t0.00906\n",
            "\"\t1000\t0.99975\n",
            "for\t2005\t0.03061\n",
            "keystone\t22271\t0.06783\n",
            "studios\t4835\t0.73596\n",
            ",\t1010\t0.99856\n",
            "charlie\t4918\t0.08171\n",
            "and\t1998\t0.99915\n",
            "his\t2010\t0.99959\n",
            "dom\t14383\t0.99924\n",
            "##ine\t3170\t0.99909\n",
            "##ering\t7999\t0.99878\n",
            "wife\t2564\t0.33281\n",
            ",\t1010\t0.99985\n",
            "mrs\t3680\t0.98233\n",
            ".\t1012\t0.99904\n",
            "sniff\t27907\t0.00270\n",
            "##les\t4244\t0.29858\n",
            ",\t1010\t0.99960\n",
            "are\t2024\t0.73862\n",
            "walking\t3788\t0.00529\n",
            "in\t1999\t0.14228\n",
            "the\t1996\t0.09168\n",
            "greens\t15505\t0.00376\n",
            "##ward\t7652\t0.00002\n",
            ".\t1012\t0.98331\n",
            "james\t2508\t0.01656\n",
            "birch\t16421\t0.00030\n",
            ",\t1010\t0.99663\n",
            "an\t2019\t0.99064\n",
            "english\t2394\t0.00949\n",
            "hunter\t4477\t0.02835\n",
            ",\t1010\t0.99525\n",
            "is\t2003\t0.99423\n",
            "accidentally\t9554\t0.30205\n",
            "shot\t2915\t0.10297\n",
            "by\t2011\t0.99303\n",
            "the\t1996\t0.25722\n",
            "servant\t7947\t0.00274\n",
            "of\t1997\t0.66885\n",
            "kirk\t11332\t0.00108\n",
            "##e\t2063\t0.01408\n",
            "warren\t6031\t0.00055\n",
            ",\t1010\t0.99428\n",
            "a\t1037\t0.96737\n",
            "wild\t3748\t0.00074\n",
            "animal\t4111\t0.04731\n",
            "painter\t5276\t0.00043\n",
            "who\t2040\t0.99241\n",
            "is\t2003\t0.90799\n",
            "camping\t13215\t0.00112\n",
            "in\t1999\t0.99184\n",
            "the\t1996\t0.92784\n",
            "jungle\t8894\t0.01240\n",
            ".\t1012\t0.99398\n",
            "king\t2332\t0.09311\n",
            "k\t1047\t0.42891\n",
            "##rew\t15603\t0.00042\n",
            "##l\t2140\t0.01568\n",
            "(\t1006\t1.00000\n",
            "raymond\t7638\t0.00001\n",
            "russell\t5735\t0.00008\n",
            ")\t1007\t1.00000\n",
            "is\t2003\t0.80853\n",
            "a\t1037\t0.78320\n",
            "cruel\t10311\t0.03907\n",
            "dictator\t21237\t0.00147\n",
            "in\t1999\t0.08083\n",
            "the\t1996\t0.97346\n",
            "emerald\t14110\t0.27746\n",
            "city\t2103\t0.54792\n",
            "in\t1999\t0.22194\n",
            "the\t1996\t0.99875\n",
            "land\t2455\t0.70094\n",
            "of\t1997\t0.99997\n",
            "oz\t11472\t0.50630\n",
            ".\t1012\t0.99502\n",
            "charlie\t4918\t0.00368\n",
            "and\t1998\t0.99904\n",
            "his\t2010\t0.95779\n",
            "friend\t2767\t0.47952\n",
            "ambrose\t15675\t0.00003\n",
            "meet\t3113\t0.83799\n",
            "in\t1999\t0.32991\n",
            "a\t1037\t0.87711\n",
            "restaurant\t4825\t0.05313\n",
            "and\t1998\t0.89499\n",
            "accidentally\t9554\t0.00204\n",
            "leave\t2681\t0.00116\n",
            "with\t2007\t0.05381\n",
            "each\t2169\t0.98265\n",
            "other\t2060\t0.99964\n",
            "'\t1005\t0.99997\n",
            "s\t1055\t0.99988\n",
            "coats\t15695\t0.00180\n",
            ".\t1012\t0.98140\n",
            "john\t2198\t0.01202\n",
            "howard\t4922\t0.11507\n",
            "payne\t13470\t0.00235\n",
            "leaves\t3727\t0.07519\n",
            "home\t2188\t0.02899\n",
            "and\t1998\t0.99789\n",
            "begins\t4269\t0.67879\n",
            "a\t1037\t0.65780\n",
            "career\t2476\t0.82882\n",
            "in\t1999\t0.98397\n",
            "the\t1996\t0.77053\n",
            "theater\t4258\t0.06516\n",
            ".\t1012\t0.98933\n",
            "im\t10047\t0.00784\n",
            "##ar\t2906\t0.01696\n",
            "the\t1996\t0.92880\n",
            "ser\t14262\t0.99924\n",
            "##vi\t5737\t0.99150\n",
            "##tor\t4263\t0.99651\n",
            "rescues\t26001\t0.00185\n",
            "an\t2019\t0.99422\n",
            "american\t2137\t0.04097\n",
            "tourist\t7538\t0.00370\n",
            "who\t2040\t0.99235\n",
            "has\t2038\t0.78749\n",
            "lost\t2439\t0.82668\n",
            "his\t2010\t0.80931\n",
            "way\t2126\t0.00407\n",
            "in\t1999\t0.84390\n",
            "the\t1996\t0.93073\n",
            "desert\t5532\t0.19023\n",
            "and\t1998\t0.37646\n",
            "the\t1996\t0.99946\n",
            "two\t2048\t0.95465\n",
            "men\t2273\t0.01570\n",
            "become\t2468\t0.96857\n",
            "friends\t2814\t0.68672\n",
            ".\t1012\t0.99607\n",
            "the\t1996\t0.99766\n",
            "following\t2206\t0.99243\n",
            "plot\t5436\t0.41789\n",
            "syn\t19962\t0.99978\n",
            "##opsis\t22599\t0.99995\n",
            "was\t2001\t0.99134\n",
            "published\t2405\t0.22285\n",
            "in\t1999\t0.99981\n",
            "conjunction\t9595\t0.53512\n",
            "with\t2007\t0.99965\n",
            "a\t1037\t0.17283\n",
            "1915\t4936\t0.00012\n",
            "showing\t4760\t0.04822\n",
            "of\t1997\t0.99928\n",
            "the\t1996\t0.98249\n",
            "film\t2143\t0.23388\n",
            "at\t2012\t0.94479\n",
            "carnegie\t11298\t0.61974\n",
            "hall\t2534\t0.99734\n",
            ":\t1024\t0.00053\n",
            "pu\t16405\t0.04768\n",
            "##g\t2290\t0.03336\n",
            ",\t1010\t0.99934\n",
            "a\t1037\t0.83836\n",
            "down\t2091\t0.84772\n",
            "-\t1011\t1.00000\n",
            "and\t1998\t0.99564\n",
            "-\t1011\t1.00000\n",
            "out\t2041\t0.59288\n",
            "ho\t7570\t0.86380\n",
            "##bo\t5092\t0.93976\n",
            ",\t1010\t0.99008\n",
            "is\t2003\t0.95645\n",
            "talked\t5720\t0.03896\n",
            "into\t2046\t0.93281\n",
            "pretending\t12097\t0.17683\n",
            "he\t2002\t0.99159\n",
            "is\t2003\t0.82508\n",
            "cyclone\t11609\t0.00001\n",
            "flynn\t13259\t0.00026\n",
            ",\t1010\t0.98663\n",
            "the\t1996\t0.23648\n",
            "boxing\t8362\t0.07979\n",
            "champion\t3410\t0.26875\n",
            ",\t1010\t0.99465\n",
            "and\t1998\t0.44727\n",
            "entering\t5738\t0.00181\n",
            "the\t1996\t0.98880\n",
            "ring\t3614\t0.41203\n",
            "for\t2005\t0.66311\n",
            "a\t1037\t0.63060\n",
            "fight\t2954\t0.70905\n",
            ".\t1012\t0.98393\n",
            "we\t2057\t0.19515\n",
            "are\t2024\t0.70688\n",
            "told\t2409\t0.42420\n",
            "charlie\t4918\t0.00011\n",
            "is\t2003\t0.73696\n",
            "a\t1037\t0.48259\n",
            "dental\t11394\t0.01676\n",
            "assistant\t3353\t0.02659\n",
            ".\t1012\t0.95287\n",
            "charlie\t4918\t0.00547\n",
            "offers\t4107\t0.95498\n",
            "mabel\t19486\t0.00010\n",
            "a\t1037\t0.98801\n",
            "ride\t4536\t0.97845\n",
            "on\t2006\t0.96055\n",
            "his\t2010\t0.78831\n",
            "two\t2048\t0.71678\n",
            "-\t1011\t0.99997\n",
            "seater\t23392\t0.06925\n",
            "motorcycle\t9055\t0.00575\n",
            ",\t1010\t0.99575\n",
            "which\t2029\t0.95578\n",
            "she\t2016\t0.28975\n",
            "accepts\t13385\t0.20259\n",
            "in\t1999\t0.99401\n",
            "preference\t12157\t0.03490\n",
            "to\t2000\t0.97485\n",
            "his\t2010\t0.11448\n",
            "rival\t6538\t0.00100\n",
            "'\t1005\t0.99997\n",
            "s\t1055\t0.99999\n",
            "racing\t3868\t0.03007\n",
            "car\t2482\t0.12229\n",
            ".\t1012\t0.98697\n",
            "mabel\t19486\t0.00103\n",
            "'\t1005\t0.99991\n",
            "s\t1055\t0.99997\n",
            "blu\t14154\t0.94818\n",
            "##nder\t11563\t0.96016\n",
            "tells\t4136\t0.36001\n",
            "the\t1996\t0.98717\n",
            "tale\t6925\t0.21562\n",
            "of\t1997\t0.99691\n",
            "a\t1037\t0.99752\n",
            "young\t2402\t0.56091\n",
            "woman\t2450\t0.63338\n",
            "who\t2040\t0.99747\n",
            "is\t2003\t0.88365\n",
            "secretly\t10082\t0.11155\n",
            "engaged\t5117\t0.49281\n",
            "to\t2000\t0.99913\n",
            "the\t1996\t0.10472\n",
            "boss\t5795\t0.00807\n",
            "'\t1005\t0.99996\n",
            "s\t1055\t0.98906\n",
            "son\t2365\t0.25148\n",
            ".\t1012\t0.98788\n",
            "[\t1031\t0.99982\n",
            "1\t1015\t0.07948\n",
            "]\t1033\t1.00000\n",
            "chaplin\t23331\t0.00031\n",
            ",\t1010\t0.99161\n",
            "in\t1999\t0.96908\n",
            "tram\t12517\t0.86777\n",
            "##p\t2361\t0.92852\n",
            "attire\t20426\t0.02021\n",
            ",\t1010\t0.99926\n",
            "sits\t7719\t0.13627\n",
            "in\t1999\t0.97492\n",
            "the\t1996\t0.65604\n",
            "park\t2380\t0.00431\n",
            "with\t2007\t0.98628\n",
            "his\t2010\t0.99976\n",
            "wife\t2564\t0.53577\n",
            ",\t1010\t0.99307\n",
            "mabel\t19486\t0.00433\n",
            ".\t1012\t0.98812\n",
            "in\t1999\t0.94781\n",
            "a\t1037\t0.44732\n",
            "hotel\t3309\t0.91869\n",
            "lobby\t9568\t0.00069\n",
            "a\t1037\t0.97401\n",
            "heavily\t4600\t0.00322\n",
            "drunk\t7144\t0.02685\n",
            "tram\t12517\t0.71410\n",
            "##p\t2361\t0.99262\n",
            "runs\t3216\t0.65653\n",
            "into\t2046\t0.66663\n",
            "an\t2019\t0.95314\n",
            "elegant\t11552\t0.00248\n",
            "lady\t3203\t0.05917\n",
            ",\t1010\t0.26895\n",
            "mabel\t19486\t0.00393\n",
            ",\t1010\t0.99691\n",
            "who\t2040\t0.95786\n",
            "gets\t4152\t0.00167\n",
            "tied\t5079\t0.06252\n",
            "up\t2039\t0.97727\n",
            "in\t1999\t0.00995\n",
            "her\t2014\t0.13211\n",
            "dog\t3899\t0.79480\n",
            "'\t1005\t0.99995\n",
            "s\t1055\t0.99999\n",
            "leash\t26834\t0.11125\n",
            ",\t1010\t0.49899\n",
            "and\t1998\t0.96213\n",
            "falls\t4212\t0.59718\n",
            "down\t2091\t0.03768\n",
            ".\t1012\t0.99047\n",
            "chaplin\t23331\t0.00241\n",
            "'\t1005\t0.99996\n",
            "s\t1055\t0.99999\n",
            "character\t2839\t0.04035\n",
            "attempts\t4740\t0.12677\n",
            "to\t2000\t0.99998\n",
            "convince\t8054\t0.20090\n",
            "a\t1037\t0.97118\n",
            "pass\t3413\t0.99825\n",
            "##er\t2121\t0.92667\n",
            "##by\t3762\t0.97899\n",
            "(\t1006\t1.00000\n",
            "director\t2472\t0.00048\n",
            "henry\t2888\t0.03260\n",
            "le\t3393\t0.17297\n",
            "##hr\t8093\t0.91276\n",
            "##man\t2386\t0.39764\n",
            ")\t1007\t1.00000\n",
            "to\t2000\t0.99983\n",
            "give\t2507\t0.67672\n",
            "him\t2032\t0.64511\n",
            "money\t2769\t0.47325\n",
            ".\t1012\t0.93165\n",
            "the\t1996\t0.93690\n",
            "mas\t16137\t0.99957\n",
            "##que\t4226\t0.99976\n",
            "##rade\t13662\t0.99618\n",
            "##r\t2099\t0.07250\n",
            "is\t2003\t0.88381\n",
            "a\t1037\t0.97791\n",
            "comedy\t4038\t0.07341\n",
            "short\t2460\t0.00587\n",
            "whose\t3005\t0.92564\n",
            "plot\t5436\t0.64314\n",
            "revolves\t19223\t0.94994\n",
            "around\t2105\t0.99138\n",
            "making\t2437\t0.02799\n",
            "films\t3152\t0.00249\n",
            "at\t2012\t0.08237\n",
            "keystone\t22271\t0.00001\n",
            ".\t1012\t0.99510\n",
            "the\t1996\t0.89701\n",
            "daughter\t2684\t0.29704\n",
            "of\t1997\t0.99876\n",
            "king\t2332\t0.96991\n",
            "neptune\t21167\t0.00359\n",
            "takes\t3138\t0.90500\n",
            "on\t2006\t0.23823\n",
            "human\t2529\t0.43999\n",
            "form\t2433\t0.88947\n",
            "to\t2000\t0.99990\n",
            "avenge\t24896\t0.97987\n",
            "the\t1996\t0.99934\n",
            "death\t2331\t0.84007\n",
            "of\t1997\t0.99988\n",
            "her\t2014\t0.97172\n",
            "young\t2402\t0.00890\n",
            "sister\t2905\t0.10404\n",
            ",\t1010\t0.91629\n",
            "who\t2040\t0.97989\n",
            "was\t2001\t0.83848\n",
            "caught\t3236\t0.67652\n",
            "in\t1999\t0.85968\n",
            "a\t1037\t0.89137\n",
            "fishing\t5645\t0.69380\n",
            "net\t5658\t0.00135\n",
            ".\t1012\t0.99202\n",
            "the\t1996\t0.90439\n",
            "hero\t5394\t0.02434\n",
            ",\t1010\t0.99849\n",
            "a\t1037\t0.86756\n",
            "jan\t5553\t0.99613\n",
            "##itor\t15660\t0.99913\n",
            "played\t2209\t0.33504\n",
            "by\t2011\t0.99986\n",
            "chaplin\t23331\t0.00166\n",
            ",\t1010\t0.99939\n",
            "is\t2003\t0.91317\n",
            "fired\t5045\t0.64883\n",
            "from\t2013\t0.98094\n",
            "work\t2147\t0.89838\n",
            "for\t2005\t0.11593\n",
            "accidentally\t9554\t0.31239\n",
            "knocking\t10591\t0.00989\n",
            "his\t2010\t0.00332\n",
            "bucket\t13610\t0.32350\n",
            "of\t1997\t0.99627\n",
            "water\t2300\t0.17097\n",
            "out\t2041\t0.53964\n",
            "the\t1996\t0.29650\n",
            "window\t3332\t0.92385\n",
            "and\t1998\t0.86605\n",
            "onto\t3031\t0.00019\n",
            "his\t2010\t0.97835\n",
            "boss\t5795\t0.52338\n",
            ",\t1010\t0.99254\n",
            "the\t1996\t0.97155\n",
            "chief\t2708\t0.00593\n",
            "banker\t13448\t0.00011\n",
            "(\t1006\t1.00000\n",
            "tan\t9092\t0.36352\n",
            "##dy\t5149\t0.02930\n",
            ")\t1007\t0.99998\n",
            ".\t1012\t0.99605\n",
            "the\t1996\t0.99291\n",
            "premise\t18458\t0.36992\n",
            "of\t1997\t0.96817\n",
            "the\t1996\t0.91983\n",
            "story\t2466\t0.39527\n",
            "was\t2001\t0.00423\n",
            "that\t2008\t0.99522\n",
            "pauline\t15595\t0.00126\n",
            "'\t1005\t0.99995\n",
            "s\t1055\t0.99999\n",
            "wealthy\t7272\t0.07764\n",
            "guardian\t6697\t0.00250\n",
            "mr\t2720\t0.81332\n",
            ".\t1012\t0.99993\n",
            "marvin\t13748\t0.00019\n",
            ",\t1010\t0.99850\n",
            "upon\t2588\t0.12013\n",
            "his\t2010\t0.69619\n",
            "death\t2331\t0.90194\n",
            ",\t1010\t0.99981\n",
            "has\t2038\t0.00711\n",
            "left\t2187\t0.86867\n",
            "her\t2014\t0.01314\n",
            "inheritance\t12839\t0.00081\n",
            "in\t1999\t0.84862\n",
            "the\t1996\t0.99981\n",
            "care\t2729\t0.37438\n",
            "of\t1997\t0.99984\n",
            "his\t2010\t0.98165\n",
            "secretary\t3187\t0.00970\n",
            ",\t1010\t0.99973\n",
            "mr\t2720\t0.00988\n",
            ".\t1012\t0.99984\n",
            "ko\t12849\t0.56756\n",
            "##ern\t11795\t0.72371\n",
            "##er\t2121\t0.93077\n",
            ",\t1010\t0.99862\n",
            "until\t2127\t0.00545\n",
            "the\t1996\t0.99961\n",
            "time\t2051\t0.19009\n",
            "of\t1997\t0.99892\n",
            "her\t2014\t0.48751\n",
            "marriage\t3510\t0.01207\n",
            ".\t1012\t0.99107\n",
            "charlie\t4918\t0.00065\n",
            "is\t2003\t0.94481\n",
            "in\t1999\t0.99923\n",
            "charge\t3715\t0.87444\n",
            "of\t1997\t0.99690\n",
            "stage\t2754\t0.00020\n",
            "\"\t1000\t0.99821\n",
            "props\t24387\t0.00215\n",
            "\"\t1000\t0.99989\n",
            "and\t1998\t0.50862\n",
            "has\t2038\t0.90417\n",
            "trouble\t4390\t0.23009\n",
            "with\t2007\t0.34601\n",
            "actors\t5889\t0.11335\n",
            "'\t1005\t0.98352\n",
            "luggage\t17434\t0.00042\n",
            "and\t1998\t0.94219\n",
            "conflicts\t9755\t0.12419\n",
            "over\t2058\t0.76081\n",
            "who\t2040\t0.99338\n",
            "gets\t4152\t0.01758\n",
            "the\t1996\t0.87530\n",
            "star\t2732\t0.00235\n",
            "'\t1005\t0.99997\n",
            "s\t1055\t0.99999\n",
            "dressing\t11225\t0.63222\n",
            "room\t2282\t0.59741\n",
            ".\t1012\t0.98999\n",
            "seated\t8901\t0.00329\n",
            "in\t1999\t0.93688\n",
            "a\t1037\t0.06391\n",
            "park\t2380\t0.00013\n",
            ",\t1010\t0.98574\n",
            "charlie\t4918\t0.00128\n",
            "gives\t3957\t0.00005\n",
            "his\t2010\t0.86650\n",
            "expert\t6739\t0.00007\n",
            "attention\t3086\t0.45055\n",
            "to\t2000\t0.99960\n",
            "the\t1996\t0.24827\n",
            "picture\t3861\t0.30222\n",
            "of\t1997\t0.99794\n",
            "a\t1037\t0.81290\n",
            "pretty\t3492\t0.00838\n",
            "girl\t2611\t0.44294\n",
            "on\t2006\t0.92666\n",
            "the\t1996\t0.99772\n",
            "cover\t3104\t0.93048\n",
            "of\t1997\t0.99958\n",
            "the\t1996\t0.97931\n",
            "police\t2610\t0.00198\n",
            "gazette\t11391\t0.01725\n",
            ".\t1012\t0.99426\n",
            "es\t9686\t0.00030\n",
            "##ra\t2527\t0.00047\n",
            "kincaid\t24510\t0.00000\n",
            "(\t1006\t0.99997\n",
            "la\t2474\t0.00004\n",
            "reno\t17738\t0.00279\n",
            ")\t1007\t0.99999\n",
            "takes\t3138\t0.27419\n",
            "land\t2455\t0.00320\n",
            "by\t2011\t0.98794\n",
            "force\t2486\t0.20156\n",
            "and\t1998\t0.81414\n",
            ",\t1010\t0.74009\n",
            "having\t2383\t0.99948\n",
            "taken\t2579\t0.04202\n",
            "the\t1996\t0.30256\n",
            "es\t9686\t0.99955\n",
            "##pin\t8091\t0.04725\n",
            "##oza\t25036\t0.16668\n",
            "land\t2455\t0.01573\n",
            ",\t1010\t0.99717\n",
            "his\t2010\t0.81265\n",
            "sights\t15925\t0.58237\n",
            "are\t2024\t0.98594\n",
            "set\t2275\t0.91141\n",
            "on\t2006\t0.97543\n",
            "the\t1996\t0.89639\n",
            "castro\t11794\t0.00007\n",
            "rancho\t18123\t0.00026\n",
            ".\t1012\t0.98903\n",
            "a\t1037\t0.87667\n",
            "drunk\t7144\t0.01528\n",
            "rev\t7065\t0.05870\n",
            "##eller\t24038\t0.00366\n",
            "(\t1006\t0.99999\n",
            "chaplin\t23331\t0.00384\n",
            ")\t1007\t1.00000\n",
            "returns\t5651\t0.64495\n",
            "home\t2188\t0.64886\n",
            "to\t2000\t0.25309\n",
            "a\t1037\t0.01846\n",
            "sc\t8040\t0.99972\n",
            "##old\t11614\t0.97742\n",
            "##ing\t2075\t0.99088\n",
            "from\t2013\t0.84730\n",
            "his\t2010\t0.99244\n",
            "wife\t2564\t0.13472\n",
            ".\t1012\t0.95598\n",
            "rough\t5931\t0.99163\n",
            "-\t1011\t1.00000\n",
            "and\t1998\t0.99596\n",
            "-\t1011\t1.00000\n",
            "tumble\t28388\t0.36815\n",
            "gold\t2751\t0.12175\n",
            "rush\t5481\t0.95899\n",
            "-\t1011\t0.99902\n",
            "era\t3690\t0.06515\n",
            "california\t2662\t0.01186\n",
            ":\t1024\t0.27101\n",
            "a\t1037\t0.97692\n",
            "woman\t2450\t0.34710\n",
            "(\t1006\t1.00000\n",
            "sal\t16183\t0.01328\n",
            "##omy\t16940\t0.00000\n",
            "jane\t4869\t0.00031\n",
            ")\t1007\t0.99999\n",
            "is\t2003\t0.97645\n",
            "saved\t5552\t0.52059\n",
            "from\t2013\t0.99793\n",
            "a\t1037\t0.97857\n",
            "ru\t21766\t0.99918\n",
            "##ffi\t26989\t0.99951\n",
            "##an\t2319\t0.99491\n",
            "(\t1006\t1.00000\n",
            "red\t2417\t0.00785\n",
            "pete\t6969\t0.00135\n",
            ")\t1007\t1.00000\n",
            "by\t2011\t0.54338\n",
            "a\t1037\t0.97336\n",
            "heroic\t14779\t0.00010\n",
            "stranger\t7985\t0.00224\n",
            "(\t1006\t1.00000\n",
            "jack\t2990\t0.08299\n",
            "dart\t14957\t0.00014\n",
            ")\t1007\t0.99999\n",
            ",\t1010\t0.78637\n",
            "the\t1996\t0.99901\n",
            "latter\t3732\t0.06317\n",
            "saved\t5552\t0.05323\n",
            "from\t2013\t0.88046\n",
            "a\t1037\t0.91212\n",
            "lynch\t11404\t0.94851\n",
            "##ing\t2075\t0.61638\n",
            "when\t2043\t0.04302\n",
            "falsely\t23123\t0.57258\n",
            "accused\t5496\t0.97343\n",
            "of\t1997\t0.99632\n",
            "a\t1037\t0.66604\n",
            "crime\t4126\t0.54414\n",
            ".\t1012\t0.99629\n",
            "my\t2026\t0.95929\n",
            "wife\t2564\t0.00927\n",
            "took\t2165\t0.07399\n",
            "me\t2033\t0.83747\n",
            "here\t2182\t0.02204\n",
            "on\t2006\t0.19012\n",
            "my\t2026\t0.87926\n",
            "birthday\t5798\t0.08880\n",
            "for\t2005\t0.91110\n",
            "breakfast\t6350\t0.03397\n",
            "and\t1998\t0.36799\n",
            "it\t2009\t0.59294\n",
            "was\t2001\t0.92122\n",
            "excellent\t6581\t0.00095\n",
            ".\t1012\t0.97706\n",
            "i\t1045\t0.97149\n",
            "have\t2031\t0.98275\n",
            "no\t2053\t0.98818\n",
            "idea\t2801\t0.94693\n",
            "why\t2339\t0.13996\n",
            "some\t2070\t0.07615\n",
            "people\t2111\t0.87895\n",
            "give\t2507\t0.43297\n",
            "bad\t2919\t0.14190\n",
            "reviews\t4391\t0.00878\n",
            "about\t2055\t0.49335\n",
            "this\t2023\t0.77653\n",
            "place\t2173\t0.08152\n",
            ".\t1012\t0.99183\n",
            "love\t2293\t0.00022\n",
            "the\t1996\t0.09171\n",
            "g\t1043\t0.76436\n",
            "##yr\t12541\t0.80777\n",
            "##o\t2080\t0.94670\n",
            "plate\t5127\t0.00025\n",
            ".\t1012\t0.87173\n",
            "rosie\t15820\t0.00006\n",
            ",\t1010\t0.76235\n",
            "dakota\t7734\t0.00002\n",
            ",\t1010\t0.96094\n",
            "and\t1998\t0.59248\n",
            "i\t1045\t0.01259\n",
            "love\t2293\t0.00144\n",
            "cha\t15775\t0.99937\n",
            "##par\t19362\t0.99960\n",
            "##ral\t7941\t0.99976\n",
            "dog\t3899\t0.00111\n",
            "park\t2380\t0.00029\n",
            "!\t999\t0.96709\n",
            "!\t999\t0.90899\n",
            "!\t999\t0.96706\n",
            "general\t2236\t0.07695\n",
            "manager\t3208\t0.00416\n",
            "scott\t3660\t0.00083\n",
            "pete\t6969\t0.00005\n",
            "##llo\t7174\t0.00030\n",
            "is\t2003\t0.03268\n",
            "a\t1037\t0.96716\n",
            "good\t2204\t0.00700\n",
            "egg\t8288\t0.00009\n",
            "!\t999\t0.09894\n",
            "!\t999\t0.60963\n",
            "!\t999\t0.91233\n",
            "qui\t21864\t0.00000\n",
            "##ess\t7971\t0.00000\n",
            "##ence\t10127\t0.00059\n",
            "is\t2003\t0.19796\n",
            ",\t1010\t0.99146\n",
            "simply\t3432\t0.95966\n",
            "put\t2404\t0.79973\n",
            ",\t1010\t0.99377\n",
            "beautiful\t3376\t0.00081\n",
            ".\t1012\t0.93433\n",
            "drop\t4530\t0.00464\n",
            "what\t2054\t0.56954\n",
            "you\t2017\t0.96092\n",
            "'\t1005\t0.99996\n",
            "re\t2128\t1.00000\n",
            "doing\t2725\t0.81187\n",
            "and\t1998\t0.71975\n",
            "drive\t3298\t0.00006\n",
            "here\t2182\t0.00074\n",
            ".\t1012\t0.97884\n",
            "luckily\t15798\t0.31982\n",
            ",\t1010\t0.99954\n",
            "i\t1045\t0.98106\n",
            "didn\t2134\t0.92322\n",
            "'\t1005\t0.99998\n",
            "t\t1056\t1.00000\n",
            "have\t2031\t0.86564\n",
            "to\t2000\t0.99985\n",
            "travel\t3604\t0.37650\n",
            "far\t2521\t0.16736\n",
            "to\t2000\t0.99900\n",
            "make\t2191\t0.17646\n",
            "my\t2026\t0.05270\n",
            "connecting\t7176\t0.00283\n",
            "flight\t3462\t0.03083\n",
            ".\t1012\t0.98210\n",
            "definitely\t5791\t0.00015\n",
            "come\t2272\t0.00005\n",
            "for\t2005\t0.01277\n",
            "happy\t3407\t0.00063\n",
            "hour\t3178\t0.09029\n",
            "!\t999\t0.02614\n",
            "no\t2053\t0.92917\n",
            "##bu\t8569\t0.92558\n",
            "##o\t2080\t0.11224\n",
            "shows\t3065\t0.00803\n",
            "his\t2010\t0.88536\n",
            "unique\t4310\t0.01232\n",
            "talents\t11725\t0.02283\n",
            "with\t2007\t0.15545\n",
            "everything\t2673\t0.09370\n",
            "on\t2006\t0.92991\n",
            "the\t1996\t0.88750\n",
            "menu\t12183\t0.07314\n",
            ".\t1012\t0.96339\n",
            "the\t1996\t0.98193\n",
            "old\t2214\t0.00365\n",
            "##ish\t4509\t0.00265\n",
            "man\t2158\t0.25865\n",
            "who\t2040\t0.92300\n",
            "owns\t8617\t0.26074\n",
            "the\t1996\t0.89831\n",
            "store\t3573\t0.03271\n",
            "is\t2003\t0.93429\n",
            "as\t2004\t0.99795\n",
            "sweet\t4086\t0.00355\n",
            "as\t2004\t0.98073\n",
            "can\t2064\t0.65569\n",
            "be\t2022\t0.88349\n",
            ".\t1012\t0.98249\n",
            "wonderful\t6919\t0.00004\n",
            "vietnamese\t9101\t0.00059\n",
            "sandwich\t11642\t0.00509\n",
            "shop\t4497\t0.23114\n",
            "##pe\t5051\t0.00068\n",
            ".\t1012\t0.88535\n",
            "they\t2027\t0.01218\n",
            "have\t2031\t0.88614\n",
            "a\t1037\t0.57704\n",
            "limited\t3132\t0.00457\n",
            "time\t2051\t0.05489\n",
            "thing\t2518\t0.01046\n",
            "going\t2183\t0.99883\n",
            "on\t2006\t0.98920\n",
            "right\t2157\t0.95313\n",
            "now\t2085\t0.99064\n",
            "with\t2007\t0.11623\n",
            "bb\t22861\t0.99593\n",
            "##q\t4160\t0.83228\n",
            "chicken\t7975\t0.00001\n",
            "pizza\t10733\t0.00198\n",
            "(\t1006\t0.99972\n",
            "not\t2025\t0.99952\n",
            "sure\t2469\t0.92428\n",
            "how\t2129\t0.99986\n",
            "long\t2146\t0.99930\n",
            "it\t2009\t0.68659\n",
            "'\t1005\t0.99990\n",
            "s\t1055\t1.00000\n",
            "going\t2183\t0.95493\n",
            "to\t2000\t0.99966\n",
            "last\t2197\t0.78258\n",
            ")\t1007\t0.98692\n",
            "but\t2021\t0.05192\n",
            "let\t2292\t0.99268\n",
            "me\t2033\t0.78970\n",
            "just\t2074\t0.99707\n",
            "say\t2360\t0.99712\n",
            "it\t2009\t0.93437\n",
            "was\t2001\t0.62016\n",
            "amazing\t6429\t0.02217\n",
            ".\t1012\t0.92861\n",
            "good\t2204\t0.00010\n",
            "tattoo\t11660\t0.00145\n",
            "shop\t4497\t0.00282\n",
            ".\t1012\t0.94627\n",
            "i\t1045\t0.99982\n",
            "'\t1005\t0.99988\n",
            "m\t1049\t0.99997\n",
            "2\t1016\t0.00099\n",
            "weeks\t3134\t0.13549\n",
            "new\t2047\t0.04023\n",
            "to\t2000\t0.45269\n",
            "phoenix\t6708\t0.00113\n",
            ".\t1012\t0.98140\n",
            "was\t2001\t0.88341\n",
            "it\t2009\t0.95524\n",
            "worth\t4276\t0.63455\n",
            "the\t1996\t0.00418\n",
            "21\t2538\t0.00001\n",
            "$\t1002\t0.00414\n",
            "for\t2005\t0.45973\n",
            "a\t1037\t0.66985\n",
            "salad\t16521\t0.01234\n",
            "and\t1998\t0.27017\n",
            "small\t2235\t0.00028\n",
            "pizza\t10733\t0.00743\n",
            "?\t1029\t0.99556\n",
            "we\t2057\t0.19698\n",
            "went\t2253\t0.00643\n",
            "here\t2182\t0.00511\n",
            "on\t2006\t0.95912\n",
            "a\t1037\t0.93187\n",
            "saturday\t5095\t0.26732\n",
            "afternoon\t5027\t0.06746\n",
            "and\t1998\t0.23243\n",
            "this\t2023\t0.19580\n",
            "place\t2173\t0.49165\n",
            "was\t2001\t0.37978\n",
            "incredibly\t11757\t0.00023\n",
            "empty\t4064\t0.00200\n",
            ".\t1012\t0.99321\n",
            "okay\t3100\t0.00005\n",
            "this\t2023\t0.88691\n",
            "is\t2003\t0.89419\n",
            "the\t1996\t0.98004\n",
            "best\t2190\t0.82300\n",
            "place\t2173\t0.00630\n",
            "ever\t2412\t0.83787\n",
            "!\t999\t0.10930\n",
            "i\t1045\t0.91790\n",
            "met\t2777\t0.32699\n",
            "a\t1037\t0.81017\n",
            "friend\t2767\t0.23797\n",
            "for\t2005\t0.67126\n",
            "lunch\t6265\t0.60667\n",
            "yesterday\t7483\t0.42433\n",
            ".\t1012\t0.99440\n",
            "they\t2027\t0.12069\n",
            "'\t1005\t0.99997\n",
            "ve\t2310\t0.92863\n",
            "gotten\t5407\t0.15354\n",
            "better\t2488\t0.99369\n",
            "and\t1998\t0.99701\n",
            "better\t2488\t0.98781\n",
            "for\t2005\t0.63701\n",
            "me\t2033\t0.78063\n",
            "in\t1999\t0.39072\n",
            "the\t1996\t0.99031\n",
            "time\t2051\t0.06048\n",
            "since\t2144\t0.84593\n",
            "this\t2023\t0.28234\n",
            "review\t3319\t0.00020\n",
            "was\t2001\t0.98948\n",
            "written\t2517\t0.27998\n",
            ".\t1012\t0.98981\n",
            "d\t1040\t0.04156\n",
            "##va\t3567\t0.00366\n",
            "##p\t2361\t0.00969\n",
            ".\t1012\t0.55946\n",
            ".\t1012\t0.96375\n",
            ".\t1012\t0.95113\n",
            ".\t1012\t0.45367\n",
            "you\t2017\t0.95304\n",
            "have\t2031\t0.62045\n",
            "to\t2000\t0.99903\n",
            "go\t2175\t0.01971\n",
            "at\t2012\t0.99929\n",
            "least\t2560\t0.92693\n",
            "once\t2320\t0.92765\n",
            "in\t1999\t0.99032\n",
            "your\t2115\t0.98918\n",
            "life\t2166\t0.82331\n",
            ".\t1012\t0.98541\n",
            "this\t2023\t0.68968\n",
            "place\t2173\t0.14440\n",
            "shouldn\t5807\t0.17089\n",
            "'\t1005\t0.99947\n",
            "t\t1056\t1.00000\n",
            "even\t2130\t0.50373\n",
            "be\t2022\t0.99554\n",
            "reviewed\t8182\t0.00003\n",
            "-\t1011\t0.00622\n",
            "because\t2138\t0.14997\n",
            "it\t2009\t0.29061\n",
            "is\t2003\t0.97274\n",
            "the\t1996\t0.99854\n",
            "kind\t2785\t0.80329\n",
            "of\t1997\t0.99896\n",
            "place\t2173\t0.82790\n",
            "i\t1045\t0.99977\n",
            "want\t2215\t0.67509\n",
            "to\t2000\t0.99991\n",
            "keep\t2562\t0.00891\n",
            "for\t2005\t0.14472\n",
            "myself\t2870\t0.61348\n",
            ".\t1012\t0.92489\n",
            ".\t1012\t0.04000\n",
            ".\t1012\t0.16336\n",
            "=\t1027\t0.00005\n",
            ")\t1007\t0.00014\n",
            "first\t2034\t0.01159\n",
            "time\t2051\t0.05610\n",
            "my\t2026\t0.28385\n",
            "friend\t2767\t0.02472\n",
            "and\t1998\t0.98747\n",
            "i\t1045\t0.98288\n",
            "went\t2253\t0.24631\n",
            "there\t2045\t0.03922\n",
            ".\t1012\t0.46366\n",
            ".\t1012\t0.30824\n",
            ".\t1012\t0.99971\n",
            "u\t1057\t0.00005\n",
            "can\t2064\t0.49993\n",
            "go\t2175\t0.24576\n",
            "there\t2045\t0.03385\n",
            "n\t1050\t0.00001\n",
            "check\t4638\t0.00983\n",
            "the\t1996\t0.25245\n",
            "car\t2482\t0.00189\n",
            "out\t2041\t0.00672\n",
            ".\t1012\t0.95511\n",
            "i\t1045\t0.98782\n",
            "love\t2293\t0.49596\n",
            "this\t2023\t0.96198\n",
            "place\t2173\t0.35847\n",
            "!\t999\t0.00572\n",
            "“\t1523\t0.00000\n",
            "i\t1045\t0.86876\n",
            "had\t2018\t0.54995\n",
            "then\t2059\t0.01842\n",
            ",\t1010\t0.99525\n",
            "as\t2004\t0.12067\n",
            "you\t2017\t0.30950\n",
            "remember\t3342\t0.00329\n",
            ",\t1010\t0.98855\n",
            "just\t2074\t0.33559\n",
            "returned\t2513\t0.24021\n",
            "to\t2000\t0.52147\n",
            "london\t2414\t0.02014\n",
            "after\t2044\t0.01147\n",
            "a\t1037\t0.99624\n",
            "lot\t2843\t0.00112\n",
            "of\t1997\t0.54260\n",
            "indian\t2796\t0.10886\n",
            "ocean\t4153\t0.87356\n",
            ",\t1010\t0.99275\n",
            "pacific\t3534\t0.16684\n",
            ",\t1010\t0.14656\n",
            "china\t2859\t0.01875\n",
            "seas\t11915\t0.00295\n",
            "-\t1011\t0.99842\n",
            "-\t1011\t0.97717\n",
            "a\t1037\t0.89513\n",
            "regular\t3180\t0.00215\n",
            "dose\t13004\t0.00098\n",
            "of\t1997\t0.80790\n",
            "the\t1996\t0.16761\n",
            "east\t2264\t0.00013\n",
            "-\t1011\t0.96092\n",
            "-\t1011\t0.94324\n",
            "six\t2416\t0.08139\n",
            "years\t2086\t0.09843\n",
            "or\t2030\t0.99866\n",
            "so\t2061\t0.74069\n",
            ",\t1010\t0.49928\n",
            "and\t1998\t0.67114\n",
            "i\t1045\t0.97805\n",
            "was\t2001\t0.84888\n",
            "loaf\t27048\t0.00217\n",
            "##ing\t2075\t0.99776\n",
            "about\t2055\t0.22048\n",
            ",\t1010\t0.69419\n",
            "hind\t17666\t0.04492\n",
            "##ering\t7999\t0.99975\n",
            "you\t2017\t0.48536\n",
            "fellows\t13572\t0.00020\n",
            "in\t1999\t0.78244\n",
            "your\t2115\t0.52779\n",
            "work\t2147\t0.07334\n",
            "and\t1998\t0.35536\n",
            "invading\t17657\t0.00029\n",
            "your\t2115\t0.97169\n",
            "homes\t5014\t0.00573\n",
            ",\t1010\t0.92058\n",
            "just\t2074\t0.00830\n",
            "as\t2004\t0.94651\n",
            "though\t2295\t0.04590\n",
            "i\t1045\t0.94226\n",
            "had\t2018\t0.83598\n",
            "got\t2288\t0.00039\n",
            "a\t1037\t0.68305\n",
            "heavenly\t16581\t0.00014\n",
            "mission\t3260\t0.03402\n",
            "to\t2000\t0.99607\n",
            "civil\t2942\t0.02641\n",
            "##ize\t4697\t0.65051\n",
            "you\t2017\t0.68077\n",
            ".\t1012\t0.96856\n",
            "he\t2002\t0.89793\n",
            "had\t2018\t0.99918\n",
            ",\t1010\t0.99983\n",
            "of\t1997\t0.99979\n",
            "course\t2607\t0.99872\n",
            ",\t1010\t0.99958\n",
            "dreamed\t13830\t0.45701\n",
            "of\t1997\t0.99115\n",
            "battles\t7465\t0.06488\n",
            "all\t2035\t0.81676\n",
            "his\t2010\t0.99989\n",
            "life\t2166\t0.99793\n",
            "-\t1011\t0.96140\n",
            "-\t1011\t0.00792\n",
            "of\t1997\t0.07337\n",
            "vague\t13727\t0.00007\n",
            "and\t1998\t0.19681\n",
            "bloody\t6703\t0.01056\n",
            "conflicts\t9755\t0.00495\n",
            "that\t2008\t0.94114\n",
            "had\t2018\t0.50690\n",
            "thrilled\t16082\t0.00461\n",
            "him\t2032\t0.96596\n",
            "with\t2007\t0.93625\n",
            "their\t2037\t0.53910\n",
            "sweep\t11740\t0.00004\n",
            "and\t1998\t0.01602\n",
            "fire\t2543\t0.00200\n",
            ".\t1012\t0.98892\n",
            "these\t2122\t0.06124\n",
            "domestic\t4968\t0.00405\n",
            "pilgrimage\t14741\t0.00000\n",
            "##s\t2015\t0.75046\n",
            "were\t2020\t0.89957\n",
            "invariably\t26597\t0.00003\n",
            "in\t1999\t0.25029\n",
            "state\t2110\t0.00001\n",
            ";\t1025\t0.00690\n",
            "two\t2048\t0.12588\n",
            "maids\t29229\t0.00417\n",
            ",\t1010\t0.00225\n",
            "the\t1996\t0.01006\n",
            "private\t2797\t0.03894\n",
            "car\t2482\t0.00005\n",
            ",\t1010\t0.15486\n",
            "or\t2030\t0.04937\n",
            "mr\t2720\t0.21618\n",
            ".\t1012\t0.99419\n",
            "blaine\t20002\t0.00017\n",
            "when\t2043\t0.00172\n",
            "available\t2800\t0.03539\n",
            ",\t1010\t0.90874\n",
            "and\t1998\t0.43812\n",
            "very\t2200\t0.06848\n",
            "often\t2411\t0.64749\n",
            "a\t1037\t0.73048\n",
            "physician\t7522\t0.00511\n",
            ".\t1012\t0.96526\n",
            "the\t1996\t0.76217\n",
            "monthly\t7058\t0.00001\n",
            "nurse\t6821\t0.00476\n",
            "tried\t2699\t0.34885\n",
            "to\t2000\t0.99996\n",
            "quiet\t4251\t0.00364\n",
            "her\t2014\t0.70330\n",
            ",\t1010\t0.89515\n",
            "and\t1998\t0.16511\n",
            "presently\t12825\t0.00005\n",
            ",\t1010\t0.92774\n",
            "from\t2013\t0.30560\n",
            "exhaustion\t15575\t0.00091\n",
            ",\t1010\t0.99799\n",
            "the\t1996\t0.48693\n",
            "crying\t6933\t0.14751\n",
            "ceased\t7024\t0.12906\n",
            ".\t1012\t0.98480\n",
            "and\t1998\t0.17562\n",
            "before\t2077\t0.78726\n",
            "we\t2057\t0.95055\n",
            "judge\t3648\t0.00014\n",
            "of\t1997\t0.09467\n",
            "them\t2068\t0.02278\n",
            "too\t2205\t0.76132\n",
            "harshly\t21052\t0.00034\n",
            "we\t2057\t0.95549\n",
            "must\t2442\t0.93496\n",
            "remember\t3342\t0.00602\n",
            "what\t2054\t0.00690\n",
            "ruthless\t18101\t0.00014\n",
            "and\t1998\t0.95432\n",
            "utter\t14395\t0.02291\n",
            "destruction\t6215\t0.31794\n",
            "our\t2256\t0.91413\n",
            "own\t2219\t0.50792\n",
            "species\t2427\t0.07834\n",
            "has\t2038\t0.87284\n",
            "wrought\t18481\t0.23160\n",
            ",\t1010\t0.79387\n",
            "not\t2025\t0.99998\n",
            "only\t2069\t0.83618\n",
            "upon\t2588\t0.85735\n",
            "animals\t4176\t0.10360\n",
            ",\t1010\t0.98719\n",
            "such\t2107\t0.99976\n",
            "as\t2004\t0.99989\n",
            "the\t1996\t0.99401\n",
            "vanished\t9955\t0.00000\n",
            "bison\t22285\t0.00594\n",
            "and\t1998\t0.82923\n",
            "the\t1996\t0.80700\n",
            "dod\t26489\t0.00402\n",
            "##o\t2080\t0.78305\n",
            ",\t1010\t0.99617\n",
            "but\t2021\t0.99705\n",
            "upon\t2588\t0.92878\n",
            "its\t2049\t0.00706\n",
            "inferior\t14092\t0.00151\n",
            "races\t3837\t0.03375\n",
            ".\t1012\t0.94321\n",
            "accordingly\t11914\t0.00059\n",
            ",\t1010\t0.98685\n",
            "with\t2007\t0.93358\n",
            "such\t2107\t0.65761\n",
            "a\t1037\t0.99043\n",
            "tram\t12517\t0.00612\n",
            "##p\t2361\t0.65188\n",
            "of\t1997\t0.38998\n",
            "his\t2010\t0.81311\n",
            "ponder\t29211\t0.64497\n",
            "##ous\t3560\t0.94067\n",
            "riding\t5559\t0.01586\n",
            "-\t1011\t0.98240\n",
            "boots\t6879\t0.03944\n",
            "as\t2004\t0.73837\n",
            "might\t2453\t0.39323\n",
            "of\t1997\t0.04612\n",
            "itself\t2993\t0.00339\n",
            "have\t2031\t0.99302\n",
            "been\t2042\t0.97112\n",
            "audible\t19525\t0.00387\n",
            "in\t1999\t0.61809\n",
            "the\t1996\t0.95900\n",
            "remote\t6556\t0.00304\n",
            "##st\t3367\t0.00058\n",
            "of\t1997\t0.99812\n",
            "the\t1996\t0.94167\n",
            "seven\t2698\t0.01232\n",
            "gables\t27008\t0.00289\n",
            ",\t1010\t0.99336\n",
            "he\t2002\t0.80382\n",
            "advanced\t3935\t0.00389\n",
            "to\t2000\t0.70115\n",
            "the\t1996\t0.97171\n",
            "door\t2341\t0.83372\n",
            ",\t1010\t0.47638\n",
            "which\t2029\t0.13353\n",
            "the\t1996\t0.39995\n",
            "servant\t7947\t0.01237\n",
            "pointed\t4197\t0.02549\n",
            "out\t2041\t0.18306\n",
            ",\t1010\t0.96553\n",
            "and\t1998\t0.89329\n",
            "made\t2081\t0.00809\n",
            "its\t2049\t0.01138\n",
            "new\t2047\t0.00051\n",
            "panels\t9320\t0.00000\n",
            "re\t2128\t0.00056\n",
            "##ech\t15937\t0.00406\n",
            "##o\t2080\t0.18278\n",
            "with\t2007\t0.68750\n",
            "a\t1037\t0.95429\n",
            "loud\t5189\t0.07711\n",
            ",\t1010\t0.67086\n",
            "free\t2489\t0.00029\n",
            "knock\t7324\t0.00621\n",
            ".\t1012\t0.93558\n",
            "‘\t1520\t0.00000\n",
            "miss\t3335\t0.15711\n",
            "tr\t19817\t0.95065\n",
            "##ot\t4140\t0.90605\n",
            "##wood\t3702\t0.57428\n",
            ",\t1010\t0.93964\n",
            "’\t1521\t0.00000\n",
            "said\t2056\t0.21762\n",
            "the\t1996\t0.90594\n",
            "visitor\t10367\t0.00221\n",
            ".\t1012\t0.99941\n",
            "\"\t1000\t0.99973\n",
            "good\t2204\t0.19298\n",
            "-\t1011\t0.79397\n",
            "morning\t2851\t0.01944\n",
            ",\t1010\t0.99219\n",
            "\"\t1000\t0.99999\n",
            "answered\t4660\t0.00148\n",
            "warwick\t13283\t0.00001\n",
            ".\t1012\t0.99954\n",
            "she\t2016\t0.95379\n",
            "had\t2018\t0.98733\n",
            ",\t1010\t0.39023\n",
            "while\t2096\t0.00071\n",
            "a\t1037\t0.99958\n",
            "very\t2200\t0.84652\n",
            "young\t2402\t0.39615\n",
            "girl\t2611\t0.27166\n",
            ",\t1010\t0.95899\n",
            "as\t2004\t0.99644\n",
            "soon\t2574\t0.00017\n",
            "as\t2004\t0.99997\n",
            "she\t2016\t0.99777\n",
            "had\t2018\t0.99975\n",
            "known\t2124\t0.11492\n",
            "him\t2032\t0.32561\n",
            "to\t2000\t0.95532\n",
            "be\t2022\t0.61857\n",
            ",\t1010\t0.48810\n",
            "in\t1999\t0.99826\n",
            "the\t1996\t0.99848\n",
            "event\t2724\t0.09338\n",
            "of\t1997\t0.99963\n",
            "her\t2014\t0.36070\n",
            "having\t2383\t0.01744\n",
            "no\t2053\t0.00177\n",
            "brother\t2567\t0.01505\n",
            ",\t1010\t0.35584\n",
            "the\t1996\t0.65899\n",
            "future\t2925\t0.01540\n",
            "baronet\t8693\t0.00065\n",
            ",\t1010\t0.00072\n",
            "meant\t3214\t0.00851\n",
            "to\t2000\t0.99989\n",
            "marry\t5914\t0.73663\n",
            "him\t2032\t0.87093\n",
            ",\t1010\t0.75437\n",
            "and\t1998\t0.08462\n",
            "her\t2014\t0.88934\n",
            "father\t2269\t0.27753\n",
            "had\t2018\t0.99884\n",
            "always\t2467\t0.27851\n",
            "meant\t3214\t0.00066\n",
            "that\t2008\t0.88183\n",
            "she\t2016\t0.89949\n",
            "should\t2323\t0.05921\n",
            ".\t1012\t0.96312\n",
            "there\t2045\t0.99442\n",
            "were\t2020\t0.43699\n",
            "mothers\t10756\t0.00734\n",
            "and\t1998\t0.28594\n",
            "brothers\t3428\t0.68540\n",
            "and\t1998\t0.96991\n",
            "sisters\t5208\t0.96903\n",
            ",\t1010\t0.98933\n",
            "and\t1998\t0.78799\n",
            "aunt\t5916\t0.66983\n",
            "##s\t2015\t0.98699\n",
            "and\t1998\t0.99506\n",
            "cousins\t12334\t0.08114\n",
            "to\t2000\t0.27442\n",
            "express\t4671\t0.21757\n",
            "various\t2536\t0.00111\n",
            "opinions\t10740\t0.46083\n",
            "on\t2006\t0.92286\n",
            "the\t1996\t0.95441\n",
            "subject\t3395\t0.06271\n",
            ",\t1010\t0.85149\n",
            "but\t2021\t0.67539\n",
            "as\t2004\t0.71351\n",
            "to\t2000\t0.80699\n",
            "what\t2054\t0.44601\n",
            "they\t2027\t0.03048\n",
            "several\t2195\t0.00001\n",
            "##ly\t2135\t0.04483\n",
            "advised\t9449\t0.00032\n",
            "history\t2381\t0.01480\n",
            "is\t2003\t0.00951\n",
            "silent\t4333\t0.00182\n",
            ".\t1012\t0.95026\n",
            "her\t2014\t0.31723\n",
            "sen\t12411\t0.99960\n",
            "##sibility\t28255\t0.74329\n",
            "prompted\t9469\t0.02652\n",
            "her\t2014\t0.98064\n",
            "to\t2000\t0.99242\n",
            "search\t3945\t0.43305\n",
            "for\t2005\t0.98662\n",
            "an\t2019\t0.85598\n",
            "object\t4874\t0.20396\n",
            "to\t2000\t0.02729\n",
            "love\t2293\t0.01847\n",
            ";\t1025\t0.31644\n",
            "on\t2006\t0.88533\n",
            "earth\t3011\t0.00213\n",
            "it\t2009\t0.76743\n",
            "was\t2001\t0.97401\n",
            "not\t2025\t0.12231\n",
            "to\t2000\t0.99970\n",
            "be\t2022\t0.99996\n",
            "found\t2179\t0.81160\n",
            ":\t1024\t0.00291\n",
            "her\t2014\t0.96516\n",
            "mother\t2388\t0.09849\n",
            "had\t2018\t0.71042\n",
            "often\t2411\t0.06889\n",
            "disappointed\t9364\t0.00310\n",
            "her\t2014\t0.98713\n",
            ",\t1010\t0.63781\n",
            "and\t1998\t0.88038\n",
            "the\t1996\t0.94998\n",
            "apparent\t6835\t0.00140\n",
            "partial\t7704\t0.04247\n",
            "##ity\t3012\t0.81893\n",
            "she\t2016\t0.54105\n",
            "she\t2016\t0.09128\n",
            "##wed\t15557\t0.00005\n",
            "to\t2000\t0.36255\n",
            "her\t2014\t0.91401\n",
            "brother\t2567\t0.00726\n",
            "gave\t2435\t0.00171\n",
            "her\t2014\t0.96245\n",
            "exquisite\t19401\t0.00347\n",
            "pain\t3255\t0.00854\n",
            "-\t1011\t0.72427\n",
            "-\t1011\t0.01667\n",
            "produced\t2550\t0.00040\n",
            "a\t1037\t0.95214\n",
            "kind\t2785\t0.16137\n",
            "of\t1997\t0.99962\n",
            "habit\t10427\t0.78768\n",
            "##ual\t8787\t0.97523\n",
            "melancholy\t22247\t0.07704\n",
            ",\t1010\t0.12803\n",
            "led\t2419\t0.03633\n",
            "her\t2014\t0.97876\n",
            "into\t2046\t0.01582\n",
            "a\t1037\t0.98203\n",
            "fond\t13545\t0.97395\n",
            "##ness\t2791\t0.99466\n",
            "for\t2005\t0.88566\n",
            "reading\t3752\t0.00023\n",
            "tales\t7122\t0.14855\n",
            "of\t1997\t0.99546\n",
            "wo\t24185\t0.36841\n",
            "##e\t2063\t0.73246\n",
            ",\t1010\t0.98054\n",
            "and\t1998\t0.98086\n",
            "made\t2081\t0.98010\n",
            "her\t2014\t0.98091\n",
            "almost\t2471\t0.00169\n",
            "realize\t5382\t0.00211\n",
            "the\t1996\t0.05902\n",
            "fictitious\t23577\t0.00002\n",
            "distress\t12893\t0.00008\n",
            ".\t1012\t0.90223\n",
            "“\t1523\t0.00000\n",
            "and\t1998\t0.24994\n",
            "so\t2061\t0.25479\n",
            "this\t2023\t0.70493\n",
            "is\t2003\t0.76922\n",
            "your\t2115\t0.68230\n",
            "first\t2034\t0.60099\n",
            "visit\t3942\t0.20464\n",
            "to\t2000\t0.98753\n",
            "chicago\t3190\t0.00937\n",
            ",\t1010\t0.98514\n",
            "”\t1524\t0.00000\n",
            "he\t2002\t0.15402\n",
            "observed\t5159\t0.00236\n",
            ".\t1012\t0.99993\n",
            "\"\t1000\t0.99872\n",
            "for\t2005\t0.05468\n",
            "whom\t3183\t0.09790\n",
            "is\t2003\t0.97851\n",
            "this\t2023\t0.18890\n",
            ",\t1010\t0.89624\n",
            "miss\t3335\t0.76295\n",
            "jem\t24193\t0.81866\n",
            "##ima\t9581\t0.13613\n",
            "?\t1029\t0.97221\n",
            "\"\t1000\t0.99998\n",
            "said\t2056\t0.28167\n",
            "miss\t3335\t0.55491\n",
            "pink\t5061\t0.68038\n",
            "##erton\t20995\t0.15144\n",
            ",\t1010\t0.99005\n",
            "with\t2007\t0.96847\n",
            "awful\t9643\t0.00108\n",
            "cold\t3147\t0.07126\n",
            "##ness\t2791\t0.59226\n",
            ".\t1012\t0.99798\n",
            "they\t2027\t0.85436\n",
            "th\t16215\t0.99996\n",
            "##rong\t17583\t0.98483\n",
            "##ed\t2098\t0.98929\n",
            ",\t1010\t0.99904\n",
            "however\t2174\t0.00794\n",
            ",\t1010\t0.99870\n",
            "to\t2000\t0.34868\n",
            "the\t1996\t0.97001\n",
            "now\t2085\t0.16988\n",
            "open\t2330\t0.64912\n",
            "door\t2341\t0.62837\n",
            ",\t1010\t0.99036\n",
            "pressing\t7827\t0.00242\n",
            "the\t1996\t0.95519\n",
            "lieutenant\t3812\t0.69929\n",
            "-\t1011\t0.99862\n",
            "governor\t3099\t0.01838\n",
            ",\t1010\t0.99904\n",
            "in\t1999\t0.14216\n",
            "the\t1996\t0.96400\n",
            "eager\t9461\t0.56706\n",
            "##ness\t2791\t0.61311\n",
            "of\t1997\t0.98705\n",
            "their\t2037\t0.02438\n",
            "curiosity\t10628\t0.01543\n",
            ",\t1010\t0.99867\n",
            "into\t2046\t0.91458\n",
            "the\t1996\t0.97709\n",
            "room\t2282\t0.21988\n",
            "in\t1999\t0.99962\n",
            "advance\t5083\t0.00003\n",
            "of\t1997\t0.98909\n",
            "them\t2068\t0.65762\n",
            ".\t1012\t0.98657\n",
            "his\t2010\t0.22076\n",
            "remark\t17674\t0.00212\n",
            "did\t2106\t0.98441\n",
            "not\t2025\t0.99967\n",
            "seem\t4025\t0.69089\n",
            "at\t2012\t0.99980\n",
            "all\t2035\t0.99964\n",
            "surprising\t11341\t0.05940\n",
            ".\t1012\t0.90412\n",
            "“\t1523\t0.00000\n",
            "and\t1998\t0.07506\n",
            "are\t2024\t0.23139\n",
            "american\t2137\t0.00016\n",
            "little\t2210\t0.06303\n",
            "boys\t3337\t0.92177\n",
            "the\t1996\t0.63235\n",
            "best\t2190\t0.00237\n",
            "little\t2210\t0.74088\n",
            "boys\t3337\t0.83731\n",
            "?\t1029\t0.46399\n",
            "”\t1524\t0.00000\n",
            "so\t2061\t0.00020\n",
            "with\t2007\t0.00012\n",
            "other\t2060\t0.00049\n",
            "vanishing\t24866\t0.00009\n",
            "##s\t2015\t0.75218\n",
            ".\t1012\t0.89438\n",
            "the\t1996\t0.12260\n",
            "search\t3945\t0.00245\n",
            "was\t2001\t0.99285\n",
            "made\t2081\t0.01485\n",
            ",\t1010\t0.84536\n",
            "and\t1998\t0.92329\n",
            "it\t2009\t0.74668\n",
            "ended\t3092\t0.12747\n",
            "-\t1011\t0.31768\n",
            "-\t1011\t0.14765\n",
            "in\t1999\t0.02393\n",
            "william\t2520\t0.00085\n",
            "dane\t14569\t0.00001\n",
            "'\t1005\t0.99966\n",
            "s\t1055\t0.97693\n",
            "finding\t4531\t0.00021\n",
            "the\t1996\t0.20712\n",
            "well\t2092\t0.10974\n",
            "-\t1011\t0.99998\n",
            "known\t2124\t0.00265\n",
            "bag\t4524\t0.00031\n",
            ",\t1010\t0.28526\n",
            "empty\t4064\t0.00038\n",
            ",\t1010\t0.38667\n",
            "tucked\t9332\t0.02570\n",
            "behind\t2369\t0.00762\n",
            "the\t1996\t0.88580\n",
            "chest\t3108\t0.97999\n",
            "of\t1997\t0.99923\n",
            "drawers\t22497\t0.84038\n",
            "in\t1999\t0.61936\n",
            "silas\t18553\t0.00004\n",
            "'\t1005\t0.99947\n",
            "s\t1055\t0.98159\n",
            "chamber\t4574\t0.00246\n",
            "!\t999\t0.00125\n",
            "there\t2045\t0.99976\n",
            "were\t2020\t0.99711\n",
            "doors\t4303\t0.96357\n",
            "all\t2035\t0.98579\n",
            "round\t2461\t0.02159\n",
            "the\t1996\t0.97924\n",
            "hall\t2534\t0.07243\n",
            ",\t1010\t0.89513\n",
            "but\t2021\t0.71666\n",
            "they\t2027\t0.92500\n",
            "were\t2020\t0.99892\n",
            "all\t2035\t0.92035\n",
            "locked\t5299\t0.63319\n",
            ";\t1025\t0.01186\n",
            "and\t1998\t0.46055\n",
            "when\t2043\t0.27443\n",
            "alice\t5650\t0.00100\n",
            "had\t2018\t0.99928\n",
            "been\t2042\t0.00417\n",
            "all\t2035\t0.99390\n",
            "the\t1996\t0.99841\n",
            "way\t2126\t0.99941\n",
            "down\t2091\t0.73321\n",
            "one\t2028\t0.99647\n",
            "side\t2217\t0.45322\n",
            "and\t1998\t0.95925\n",
            "up\t2039\t0.59189\n",
            "the\t1996\t0.99915\n",
            "other\t2060\t0.94806\n",
            ",\t1010\t0.94836\n",
            "trying\t2667\t0.02228\n",
            "every\t2296\t0.19439\n",
            "door\t2341\t0.42855\n",
            ",\t1010\t0.99191\n",
            "she\t2016\t0.97517\n",
            "walked\t2939\t0.06546\n",
            "sadly\t13718\t0.00080\n",
            "down\t2091\t0.42718\n",
            "the\t1996\t0.99521\n",
            "middle\t2690\t0.00006\n",
            ",\t1010\t0.99262\n",
            "wondering\t6603\t0.95580\n",
            "how\t2129\t0.16219\n",
            "she\t2016\t0.99195\n",
            "was\t2001\t0.84918\n",
            "ever\t2412\t0.00015\n",
            "to\t2000\t0.56309\n",
            "get\t2131\t0.21323\n",
            "out\t2041\t0.29664\n",
            "again\t2153\t0.56056\n",
            ".\t1012\t0.97872\n",
            "ogden\t23203\t0.00001\n",
            "ford\t4811\t0.00005\n",
            "was\t2001\t0.90619\n",
            "sprawling\t24199\t0.00003\n",
            "in\t1999\t0.90454\n",
            "a\t1037\t0.84483\n",
            "deep\t2784\t0.00024\n",
            "chair\t3242\t0.07828\n",
            "in\t1999\t0.95796\n",
            "the\t1996\t0.99915\n",
            "shadows\t6281\t0.00363\n",
            ".\t1012\t0.99139\n",
            "the\t1996\t0.56402\n",
            "school\t2082\t0.78378\n",
            "##master\t8706\t0.81160\n",
            "is\t2003\t0.04908\n",
            "generally\t3227\t0.02771\n",
            "a\t1037\t0.98651\n",
            "man\t2158\t0.25466\n",
            "of\t1997\t0.99827\n",
            "some\t2070\t0.09648\n",
            "importance\t5197\t0.04535\n",
            "in\t1999\t0.60798\n",
            "the\t1996\t0.95841\n",
            "female\t2931\t0.00002\n",
            "circle\t4418\t0.00715\n",
            "of\t1997\t0.92228\n",
            "a\t1037\t0.26492\n",
            "rural\t3541\t0.06229\n",
            "neighborhood\t5101\t0.00078\n",
            ";\t1025\t0.00694\n",
            "being\t2108\t0.00771\n",
            "considered\t2641\t0.17740\n",
            "a\t1037\t0.89893\n",
            "kind\t2785\t0.02898\n",
            "of\t1997\t0.28115\n",
            "idle\t18373\t0.00018\n",
            ",\t1010\t0.36856\n",
            "gentleman\t10170\t0.00872\n",
            "##like\t10359\t0.00598\n",
            "persona\t16115\t0.98243\n",
            "##ge\t3351\t0.99819\n",
            ",\t1010\t0.95436\n",
            "of\t1997\t0.90475\n",
            "vastly\t24821\t0.00609\n",
            "superior\t6020\t0.78188\n",
            "taste\t5510\t0.00202\n",
            "and\t1998\t0.98537\n",
            "accomplishments\t17571\t0.00022\n",
            "to\t2000\t0.32692\n",
            "the\t1996\t0.88652\n",
            "rough\t5931\t0.00119\n",
            "country\t2406\t0.07880\n",
            "sw\t25430\t0.36920\n",
            "##ains\t28247\t0.02847\n",
            ",\t1010\t0.91280\n",
            "and\t1998\t0.91348\n",
            ",\t1010\t0.99538\n",
            "indeed\t5262\t0.05658\n",
            ",\t1010\t0.99902\n",
            "inferior\t14092\t0.05829\n",
            "in\t1999\t0.98200\n",
            "learning\t4083\t0.00021\n",
            "only\t2069\t0.01165\n",
            "to\t2000\t0.89119\n",
            "the\t1996\t0.77900\n",
            "par\t11968\t0.99746\n",
            "##son\t3385\t0.83049\n",
            ".\t1012\t0.97517\n",
            "\"\t1000\t0.00121\n",
            "my\t2026\t0.76161\n",
            "friend\t2767\t0.05113\n",
            ",\t1010\t0.89342\n",
            "forget\t5293\t0.00025\n",
            "your\t2115\t0.84668\n",
            "resentment\t20234\t0.00048\n",
            ",\t1010\t0.18140\n",
            "in\t1999\t0.97360\n",
            "favour\t7927\t0.00202\n",
            "of\t1997\t0.98153\n",
            "your\t2115\t0.63554\n",
            "humanity\t8438\t0.00031\n",
            ";\t1025\t0.00131\n",
            "-\t1011\t0.00066\n",
            "a\t1037\t0.01267\n",
            "father\t2269\t0.29098\n",
            ",\t1010\t0.97043\n",
            "trembling\t10226\t0.00002\n",
            "for\t2005\t0.60914\n",
            "the\t1996\t0.99956\n",
            "welfare\t7574\t0.02456\n",
            "of\t1997\t0.99978\n",
            "his\t2010\t0.77707\n",
            "child\t2775\t0.25735\n",
            ",\t1010\t0.99407\n",
            "be\t2022\t0.99975\n",
            "##que\t4226\t0.99859\n",
            "##ath\t8988\t0.99732\n",
            "##s\t2015\t0.96919\n",
            "her\t2014\t0.00772\n",
            "to\t2000\t0.62155\n",
            "your\t2115\t0.10335\n",
            "care\t2729\t0.04568\n",
            ".\t1012\t0.63596\n",
            "he\t2002\t0.82822\n",
            "was\t2001\t0.92603\n",
            "a\t1037\t0.99972\n",
            "very\t2200\t0.33093\n",
            "silent\t4333\t0.00008\n",
            "man\t2158\t0.62027\n",
            "by\t2011\t0.73618\n",
            "custom\t7661\t0.00007\n",
            ".\t1012\t0.91472\n",
            "“\t1523\t0.00000\n",
            "i\t1045\t0.78151\n",
            "never\t2196\t0.81452\n",
            "did\t2106\t0.80751\n",
            "see\t2156\t0.01090\n",
            "the\t1996\t0.52520\n",
            "beat\t3786\t0.00003\n",
            "of\t1997\t0.23666\n",
            "that\t2008\t0.19272\n",
            "boy\t2879\t0.00062\n",
            "!\t999\t0.00303\n",
            "”\t1524\t0.00000\n",
            "\"\t1000\t0.07020\n",
            "mercy\t8673\t0.00528\n",
            "on\t2006\t0.06172\n",
            "us\t2149\t0.10751\n",
            ",\t1010\t0.97532\n",
            "good\t2204\t0.13846\n",
            "##wife\t19993\t0.00301\n",
            "!\t999\t0.43220\n",
            "\"\t1000\t0.99953\n",
            "exclaimed\t12713\t0.01879\n",
            "a\t1037\t0.62585\n",
            "man\t2158\t0.05654\n",
            "in\t1999\t0.57730\n",
            "the\t1996\t0.99715\n",
            "crowd\t4306\t0.21557\n",
            ",\t1010\t0.11542\n",
            "\"\t1000\t0.99539\n",
            "is\t2003\t0.99227\n",
            "there\t2045\t0.99504\n",
            "no\t2053\t0.27591\n",
            "virtue\t11870\t0.02532\n",
            "in\t1999\t0.76737\n",
            "woman\t2450\t0.00515\n",
            ",\t1010\t0.82238\n",
            "save\t3828\t0.00395\n",
            "what\t2054\t0.21187\n",
            "springs\t6076\t0.00305\n",
            "from\t2013\t0.94771\n",
            "a\t1037\t0.06074\n",
            "whole\t2878\t0.37676\n",
            "##some\t14045\t0.28526\n",
            "fear\t3571\t0.02946\n",
            "of\t1997\t0.80577\n",
            "the\t1996\t0.98606\n",
            "gallo\t25624\t0.94492\n",
            "##ws\t9333\t0.99987\n",
            "?\t1029\t0.85529\n",
            "that\t2008\t0.31080\n",
            "is\t2003\t0.91065\n",
            "the\t1996\t0.92708\n",
            "hardest\t18263\t0.02371\n",
            "word\t2773\t0.00849\n",
            "yet\t2664\t0.04412\n",
            "!\t999\t0.44336\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00002\n",
            "white\t2317\t0.12294\n",
            "##house\t4580\t0.19932\n",
            ":\t1024\t0.62922\n",
            "since\t2144\t0.28262\n",
            "august\t2257\t0.11618\n",
            "14\t2403\t0.00651\n",
            ",\t1010\t0.99866\n",
            "the\t1996\t0.99793\n",
            "u\t1057\t0.99997\n",
            ".\t1012\t0.99999\n",
            "s\t1055\t0.98877\n",
            ".\t1012\t0.99762\n",
            "has\t2038\t0.59037\n",
            "evacuated\t13377\t0.00505\n",
            "and\t1998\t0.93362\n",
            "facilitated\t19601\t0.01743\n",
            "the\t1996\t0.99648\n",
            "evacuation\t13982\t0.92299\n",
            "of\t1997\t0.99964\n",
            "approximately\t3155\t0.24999\n",
            "116\t12904\t0.00016\n",
            ",\t1010\t0.99182\n",
            "700\t6352\t0.00191\n",
            "people\t2111\t0.59687\n",
            ".\t1012\t0.99872\n",
            "rt\t19387\t0.00004\n",
            "@\t1030\t0.00012\n",
            "white\t2317\t0.18046\n",
            "##house\t4580\t0.10399\n",
            ":\t1024\t0.42496\n",
            "from\t2013\t0.92717\n",
            "3\t1017\t0.03099\n",
            "am\t2572\t0.03254\n",
            "et\t3802\t0.98941\n",
            "on\t2006\t0.99663\n",
            "8\t1022\t0.10267\n",
            "/\t1013\t0.99991\n",
            "29\t2756\t0.20932\n",
            "to\t2000\t0.86292\n",
            "3a\t23842\t0.07763\n",
            "##m\t2213\t0.66167\n",
            "et\t3802\t0.99867\n",
            "on\t2006\t0.99711\n",
            "8\t1022\t0.21984\n",
            "/\t1013\t0.99951\n",
            "30\t2382\t0.25872\n",
            ",\t1010\t0.62348\n",
            "a\t1037\t0.99991\n",
            "total\t2561\t0.98544\n",
            "of\t1997\t0.99987\n",
            "approximately\t3155\t0.13491\n",
            "1\t1015\t0.37809\n",
            ",\t1010\t0.99859\n",
            "200\t3263\t0.10699\n",
            "people\t2111\t0.85062\n",
            "were\t2020\t0.99300\n",
            "evacuated\t13377\t0.70775\n",
            "from\t2013\t0.82755\n",
            "kabul\t21073\t0.00072\n",
            ".\t1012\t0.99458\n",
            "thanks\t4283\t0.80995\n",
            "to\t2000\t0.99629\n",
            "the\t1996\t0.99876\n",
            "hard\t2524\t0.15869\n",
            "work\t2147\t0.88715\n",
            "of\t1997\t0.71367\n",
            "@\t1030\t0.00001\n",
            "fe\t10768\t0.13417\n",
            "##ma\t2863\t0.27928\n",
            ",\t1010\t0.98760\n",
            "we\t2057\t0.54561\n",
            "’\t1521\t0.00000\n",
            "ve\t2310\t0.00011\n",
            "pre\t3653\t0.00988\n",
            "-\t1011\t0.99982\n",
            "positioned\t10959\t0.00410\n",
            "resources\t4219\t0.01968\n",
            ",\t1010\t0.99857\n",
            "equipment\t3941\t0.06823\n",
            ",\t1010\t0.99835\n",
            "and\t1998\t0.99919\n",
            "response\t3433\t0.07463\n",
            "teams\t2780\t0.47198\n",
            "to\t2000\t0.99672\n",
            "respond\t6869\t0.98671\n",
            "to\t2000\t0.98871\n",
            "hurricane\t7064\t0.00278\n",
            "ida\t16096\t0.00162\n",
            ".\t1012\t0.99591\n",
            "our\t2256\t0.01828\n",
            "whole\t2878\t0.00163\n",
            "-\t1011\t0.05135\n",
            "of\t1997\t0.01553\n",
            "-\t1011\t0.99955\n",
            "government\t2231\t0.00071\n",
            "effort\t3947\t0.00205\n",
            "is\t2003\t0.34007\n",
            "already\t2525\t0.01226\n",
            "hard\t2524\t0.12413\n",
            "at\t2012\t0.95278\n",
            "work\t2147\t0.97762\n",
            ".\t1012\t0.96239\n",
            "the\t1996\t0.93174\n",
            "13\t2410\t0.00063\n",
            "service\t2326\t0.01327\n",
            "members\t2372\t0.81636\n",
            "that\t2008\t0.63408\n",
            "we\t2057\t0.04120\n",
            "lost\t2439\t0.09121\n",
            "were\t2020\t0.51884\n",
            "heroes\t7348\t0.00075\n",
            "who\t2040\t0.89521\n",
            "made\t2081\t0.97574\n",
            "the\t1996\t0.96119\n",
            "ultimate\t7209\t0.19799\n",
            "sacrifice\t8688\t0.83722\n",
            "in\t1999\t0.99555\n",
            "service\t2326\t0.01757\n",
            "of\t1997\t0.03657\n",
            "our\t2256\t0.11228\n",
            "highest\t3284\t0.00212\n",
            "american\t2137\t0.01374\n",
            "ideals\t15084\t0.38932\n",
            "and\t1998\t0.00906\n",
            "while\t2096\t0.00664\n",
            "saving\t7494\t0.86889\n",
            "the\t1996\t0.99960\n",
            "lives\t3268\t0.99078\n",
            "of\t1997\t0.99994\n",
            "others\t2500\t0.86891\n",
            ".\t1012\t0.99559\n",
            "to\t2000\t0.40389\n",
            "the\t1996\t0.97691\n",
            "people\t2111\t0.37652\n",
            "of\t1997\t0.95119\n",
            "the\t1996\t0.99178\n",
            "gulf\t6084\t0.01191\n",
            "coast\t3023\t0.30783\n",
            ":\t1024\t0.02150\n",
            "please\t3531\t0.14728\n",
            "follow\t3582\t0.92960\n",
            "the\t1996\t0.99599\n",
            "instructions\t8128\t0.24729\n",
            "of\t1997\t0.98686\n",
            "local\t2334\t0.08603\n",
            "officials\t4584\t0.05931\n",
            "during\t2076\t0.25777\n",
            "this\t2023\t0.79965\n",
            "dangerous\t4795\t0.00125\n",
            "time\t2051\t0.20728\n",
            ".\t1012\t0.98994\n",
            "today\t2651\t0.00789\n",
            "i\t1045\t0.92996\n",
            "was\t2001\t0.76391\n",
            "brief\t4766\t0.99442\n",
            "##ed\t2098\t0.99994\n",
            "on\t2006\t0.89962\n",
            "our\t2256\t0.03451\n",
            "preparations\t12929\t0.36296\n",
            "for\t2005\t0.95984\n",
            "hurricane\t7064\t0.00285\n",
            "ida\t16096\t0.00049\n",
            "by\t2011\t0.00068\n",
            "@\t1030\t0.00000\n",
            "fe\t10768\t0.01560\n",
            "##ma\t2863\t0.04654\n",
            ".\t1012\t0.96306\n",
            "i\t1045\t0.22802\n",
            "said\t2056\t0.34138\n",
            "we\t2057\t0.84110\n",
            "would\t2052\t0.55331\n",
            "go\t2175\t0.71649\n",
            "after\t2044\t0.81029\n",
            "the\t1996\t0.98145\n",
            "group\t2177\t0.00985\n",
            "responsible\t3625\t0.99861\n",
            "for\t2005\t0.99861\n",
            "the\t1996\t0.95380\n",
            "attack\t2886\t0.32892\n",
            "on\t2006\t0.63442\n",
            "our\t2256\t0.32382\n",
            "troops\t3629\t0.00815\n",
            "and\t1998\t0.71503\n",
            "innocent\t7036\t0.00899\n",
            "civilians\t9272\t0.65310\n",
            "in\t1999\t0.97342\n",
            "kabul\t21073\t0.00445\n",
            ",\t1010\t0.39284\n",
            "and\t1998\t0.17181\n",
            "we\t2057\t0.58955\n",
            "have\t2031\t0.00020\n",
            ".\t1012\t0.97341\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00001\n",
            "white\t2317\t0.11368\n",
            "##house\t4580\t0.19556\n",
            ":\t1024\t0.61645\n",
            "since\t2144\t0.32106\n",
            "august\t2257\t0.11751\n",
            "14\t2403\t0.00523\n",
            ",\t1010\t0.99855\n",
            "the\t1996\t0.99737\n",
            "u\t1057\t0.99997\n",
            ".\t1012\t0.99999\n",
            "s\t1055\t0.99001\n",
            ".\t1012\t0.99804\n",
            "has\t2038\t0.58999\n",
            "evacuated\t13377\t0.00539\n",
            "and\t1998\t0.92880\n",
            "facilitated\t19601\t0.01726\n",
            "the\t1996\t0.99611\n",
            "evacuation\t13982\t0.91887\n",
            "of\t1997\t0.99963\n",
            "approximately\t3155\t0.23922\n",
            "111\t11118\t0.00010\n",
            ",\t1010\t0.99014\n",
            "900\t7706\t0.00059\n",
            "people\t2111\t0.62489\n",
            ".\t1012\t0.99860\n",
            "rt\t19387\t0.00010\n",
            "@\t1030\t0.00025\n",
            "white\t2317\t0.23252\n",
            "##house\t4580\t0.09871\n",
            ":\t1024\t0.01982\n",
            "update\t10651\t0.00051\n",
            ":\t1024\t0.53900\n",
            "from\t2013\t0.95279\n",
            "3\t1017\t0.06932\n",
            "am\t2572\t0.24526\n",
            "et\t3802\t0.99765\n",
            "on\t2006\t0.99758\n",
            "8\t1022\t0.30019\n",
            "/\t1013\t0.99993\n",
            "27\t2676\t0.32495\n",
            "to\t2000\t0.91581\n",
            "3\t1017\t0.06334\n",
            "am\t2572\t0.05739\n",
            "et\t3802\t0.99821\n",
            "on\t2006\t0.99762\n",
            "8\t1022\t0.44921\n",
            "/\t1013\t0.99973\n",
            "28\t2654\t0.24390\n",
            ",\t1010\t0.60077\n",
            "a\t1037\t0.99991\n",
            "total\t2561\t0.99192\n",
            "of\t1997\t0.99981\n",
            "approximately\t3155\t0.13683\n",
            "6\t1020\t0.04615\n",
            ",\t1010\t0.99519\n",
            "800\t5385\t0.02401\n",
            "people\t2111\t0.89896\n",
            "were\t2020\t0.99156\n",
            "evacuated\t13377\t0.78378\n",
            "from\t2013\t0.84114\n",
            "kabul\t21073\t0.00308\n",
            ".\t1012\t0.99408\n",
            "this\t2023\t0.12108\n",
            "afternoon\t5027\t0.18453\n",
            ",\t1010\t0.99853\n",
            "i\t1045\t0.00280\n",
            "held\t2218\t0.01836\n",
            "a\t1037\t0.99212\n",
            "call\t2655\t0.00023\n",
            "with\t2007\t0.19326\n",
            "the\t1996\t0.99732\n",
            "head\t2132\t0.17810\n",
            "of\t1997\t0.99799\n",
            "fe\t10768\t0.92140\n",
            "##ma\t2863\t0.83336\n",
            "and\t1998\t0.13000\n",
            "governors\t11141\t0.00005\n",
            "ahead\t3805\t0.00393\n",
            "of\t1997\t0.95012\n",
            "hurricane\t7064\t0.50212\n",
            "ida\t16096\t0.00252\n",
            "to\t2000\t0.99884\n",
            "discuss\t6848\t0.13915\n",
            "preparations\t12929\t0.37367\n",
            "for\t2005\t0.99320\n",
            "what\t2054\t0.99996\n",
            "is\t2003\t0.00784\n",
            "expected\t3517\t0.17008\n",
            "to\t2000\t0.99996\n",
            "be\t2022\t0.96257\n",
            "a\t1037\t0.99131\n",
            "dangerous\t4795\t0.00359\n",
            "storm\t4040\t0.38849\n",
            ".\t1012\t0.99812\n",
            "it\t2009\t0.99474\n",
            "was\t2001\t0.47283\n",
            "an\t2019\t0.98098\n",
            "honor\t3932\t0.86395\n",
            "to\t2000\t0.99959\n",
            "welcome\t6160\t0.50039\n",
            "israeli\t5611\t0.09028\n",
            "prime\t3539\t0.97113\n",
            "minister\t2704\t0.99984\n",
            "na\t6583\t0.99589\n",
            "##ft\t6199\t0.99745\n",
            "##ali\t11475\t0.99858\n",
            "bennett\t8076\t0.00062\n",
            "to\t2000\t0.67270\n",
            "the\t1996\t0.99780\n",
            "white\t2317\t0.92757\n",
            "house\t2160\t0.99939\n",
            "today\t2651\t0.17991\n",
            ".\t1012\t0.99089\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00001\n",
            "white\t2317\t0.11720\n",
            "##house\t4580\t0.18847\n",
            ":\t1024\t0.63982\n",
            "since\t2144\t0.33577\n",
            "august\t2257\t0.11387\n",
            "14\t2403\t0.00514\n",
            ",\t1010\t0.99873\n",
            "the\t1996\t0.99734\n",
            "u\t1057\t0.99997\n",
            ".\t1012\t0.99999\n",
            "s\t1055\t0.98846\n",
            ".\t1012\t0.99783\n",
            "has\t2038\t0.64359\n",
            "evacuated\t13377\t0.00513\n",
            "and\t1998\t0.94677\n",
            "facilitated\t19601\t0.02402\n",
            "the\t1996\t0.99552\n",
            "evacuation\t13982\t0.92507\n",
            "of\t1997\t0.99960\n",
            "approximately\t3155\t0.14232\n",
            "105\t8746\t0.00113\n",
            ",\t1010\t0.99918\n",
            "000\t2199\t0.98055\n",
            "people\t2111\t0.61281\n",
            ".\t1012\t0.99873\n",
            "rt\t19387\t0.00012\n",
            "@\t1030\t0.00022\n",
            "white\t2317\t0.26011\n",
            "##house\t4580\t0.09163\n",
            ":\t1024\t0.01788\n",
            "update\t10651\t0.00039\n",
            ":\t1024\t0.11868\n",
            "from\t2013\t0.93213\n",
            "3\t1017\t0.05929\n",
            "am\t2572\t0.24517\n",
            "et\t3802\t0.99757\n",
            "on\t2006\t0.99665\n",
            "8\t1022\t0.19537\n",
            "/\t1013\t0.99990\n",
            "26\t2656\t0.16309\n",
            "to\t2000\t0.92211\n",
            "3\t1017\t0.06000\n",
            "am\t2572\t0.06740\n",
            "et\t3802\t0.99857\n",
            "on\t2006\t0.99790\n",
            "8\t1022\t0.40611\n",
            "/\t1013\t0.99982\n",
            "27\t2676\t0.14239\n",
            "a\t1037\t0.99450\n",
            "total\t2561\t0.98636\n",
            "of\t1997\t0.99941\n",
            "approximately\t3155\t0.12905\n",
            "12\t2260\t0.01434\n",
            ",\t1010\t0.99294\n",
            "500\t3156\t0.05708\n",
            "people\t2111\t0.89275\n",
            "were\t2020\t0.98892\n",
            "evacuated\t13377\t0.87918\n",
            "from\t2013\t0.84698\n",
            "kabul\t21073\t0.00297\n",
            ".\t1012\t0.99284\n",
            "the\t1996\t0.27547\n",
            "american\t2137\t0.03128\n",
            "service\t2326\t0.02704\n",
            "members\t2372\t0.25460\n",
            "who\t2040\t0.97393\n",
            "gave\t2435\t0.14442\n",
            "their\t2037\t0.99873\n",
            "lives\t3268\t0.95675\n",
            "were\t2020\t0.40440\n",
            "heroes\t7348\t0.00223\n",
            ".\t1012\t0.92489\n",
            "watch\t3422\t0.11663\n",
            "as\t2004\t0.01891\n",
            "i\t1045\t0.05683\n",
            "deliver\t8116\t0.09190\n",
            "remarks\t12629\t0.00135\n",
            "on\t2006\t0.55406\n",
            "the\t1996\t0.97721\n",
            "terror\t7404\t0.21451\n",
            "attack\t2886\t0.22701\n",
            "at\t2012\t0.08945\n",
            "hamid\t24811\t0.96834\n",
            "ka\t10556\t0.99841\n",
            "##rza\t24175\t0.98903\n",
            "##i\t2072\t0.99998\n",
            "international\t2248\t0.99879\n",
            "airport\t3199\t0.98989\n",
            ",\t1010\t0.83361\n",
            "and\t1998\t0.03692\n",
            "the\t1996\t0.22833\n",
            "u\t1057\t0.99920\n",
            ".\t1012\t0.99997\n",
            "s\t1055\t0.99790\n",
            ".\t1012\t0.99778\n",
            "service\t2326\t0.02392\n",
            "members\t2372\t0.18124\n",
            "and\t1998\t0.69474\n",
            "afghan\t12632\t0.00314\n",
            "victims\t5694\t0.00069\n",
            "killed\t2730\t0.76495\n",
            "and\t1998\t0.86640\n",
            "wounded\t5303\t0.55135\n",
            ".\t1012\t0.97397\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00002\n",
            "white\t2317\t0.10674\n",
            "##house\t4580\t0.19349\n",
            ":\t1024\t0.64099\n",
            "since\t2144\t0.27367\n",
            "august\t2257\t0.11624\n",
            "14\t2403\t0.00584\n",
            ",\t1010\t0.99873\n",
            "the\t1996\t0.99760\n",
            "u\t1057\t0.99997\n",
            ".\t1012\t0.99999\n",
            "s\t1055\t0.98813\n",
            ".\t1012\t0.99754\n",
            "has\t2038\t0.54845\n",
            "evacuated\t13377\t0.00510\n",
            "and\t1998\t0.93833\n",
            "facilitated\t19601\t0.01754\n",
            "the\t1996\t0.99590\n",
            "evacuation\t13982\t0.92248\n",
            "of\t1997\t0.99961\n",
            "approximately\t3155\t0.19372\n",
            "95\t5345\t0.00025\n",
            ",\t1010\t0.98207\n",
            "700\t6352\t0.00212\n",
            "people\t2111\t0.60530\n",
            ".\t1012\t0.99878\n",
            "rt\t19387\t0.00026\n",
            "@\t1030\t0.00050\n",
            "white\t2317\t0.23120\n",
            "##house\t4580\t0.07910\n",
            ":\t1024\t0.02124\n",
            "update\t10651\t0.00267\n",
            ":\t1024\t0.58247\n",
            "from\t2013\t0.94639\n",
            "3\t1017\t0.02919\n",
            "am\t2572\t0.04591\n",
            "et\t3802\t0.98788\n",
            "on\t2006\t0.99434\n",
            "8\t1022\t0.07004\n",
            "/\t1013\t0.99990\n",
            "25\t2423\t0.31539\n",
            "to\t2000\t0.85597\n",
            "3a\t23842\t0.07415\n",
            "##m\t2213\t0.81987\n",
            "et\t3802\t0.99868\n",
            "on\t2006\t0.99705\n",
            "8\t1022\t0.15782\n",
            "/\t1013\t0.99971\n",
            "26\t2656\t0.28322\n",
            ",\t1010\t0.63595\n",
            "a\t1037\t0.99992\n",
            "total\t2561\t0.99302\n",
            "of\t1997\t0.99981\n",
            "approximately\t3155\t0.19990\n",
            "13\t2410\t0.00973\n",
            ",\t1010\t0.99639\n",
            "400\t4278\t0.01605\n",
            "people\t2111\t0.58097\n",
            "were\t2020\t0.99028\n",
            "evacuated\t13377\t0.83068\n",
            "from\t2013\t0.63182\n",
            "afghanistan\t7041\t0.00381\n",
            ".\t1012\t0.99509\n",
            "tune\t8694\t0.00694\n",
            "in\t1999\t0.85408\n",
            "as\t2004\t0.17855\n",
            "i\t1045\t0.01588\n",
            "discuss\t6848\t0.03092\n",
            "the\t1996\t0.89610\n",
            "whole\t2878\t0.00236\n",
            "-\t1011\t0.98454\n",
            "of\t1997\t0.06535\n",
            "-\t1011\t0.99777\n",
            "nation\t3842\t0.00099\n",
            "effort\t3947\t0.02676\n",
            "required\t3223\t0.20071\n",
            "to\t2000\t0.99985\n",
            "improve\t5335\t0.08973\n",
            "our\t2256\t0.34094\n",
            "cyber\t16941\t0.12878\n",
            "##se\t3366\t0.99973\n",
            "##cu\t10841\t0.99689\n",
            "##rity\t15780\t0.99990\n",
            ".\t1012\t0.96403\n",
            "rt\t19387\t0.00003\n",
            "@\t1030\t0.00002\n",
            "white\t2317\t0.11383\n",
            "##house\t4580\t0.11567\n",
            ":\t1024\t0.35446\n",
            "since\t2144\t0.23712\n",
            "august\t2257\t0.10794\n",
            "14\t2403\t0.00400\n",
            ",\t1010\t0.99172\n",
            "the\t1996\t0.96731\n",
            "u\t1057\t0.99984\n",
            ".\t1012\t0.99997\n",
            "s\t1055\t0.98325\n",
            ".\t1012\t0.99730\n",
            "has\t2038\t0.69817\n",
            "evacuated\t13377\t0.00063\n",
            "and\t1998\t0.92253\n",
            "facilitated\t19601\t0.03593\n",
            "the\t1996\t0.97642\n",
            "evacuation\t13982\t0.31894\n",
            "of\t1997\t0.99937\n",
            "approximately\t3155\t0.17155\n",
            "82\t6445\t0.00035\n",
            ",\t1010\t0.96278\n",
            "300\t3998\t0.00435\n",
            "people\t2111\t0.21175\n",
            "on\t2006\t0.00044\n",
            "us\t2149\t0.00266\n",
            "military\t2510\t0.00008\n",
            "and\t1998\t0.00604\n",
            "co\t2522\t0.00000\n",
            "…\t1529\t0.00000\n",
            "rt\t19387\t0.00009\n",
            "@\t1030\t0.00023\n",
            "white\t2317\t0.23760\n",
            "##house\t4580\t0.07661\n",
            ":\t1024\t0.01792\n",
            "update\t10651\t0.00061\n",
            ":\t1024\t0.51956\n",
            "from\t2013\t0.94651\n",
            "3\t1017\t0.05751\n",
            "am\t2572\t0.22501\n",
            "et\t3802\t0.99816\n",
            "on\t2006\t0.99727\n",
            "8\t1022\t0.03420\n",
            "/\t1013\t0.99991\n",
            "24\t2484\t0.22854\n",
            "to\t2000\t0.90279\n",
            "3\t1017\t0.06441\n",
            "am\t2572\t0.06208\n",
            "et\t3802\t0.99835\n",
            "on\t2006\t0.99776\n",
            "8\t1022\t0.06443\n",
            "/\t1013\t0.99967\n",
            "25\t2423\t0.09212\n",
            ",\t1010\t0.63233\n",
            "approximately\t3155\t0.21879\n",
            "19\t2539\t0.00348\n",
            ",\t1010\t0.99194\n",
            "000\t2199\t0.90564\n",
            "people\t2111\t0.82661\n",
            "were\t2020\t0.98253\n",
            "evacuated\t13377\t0.84972\n",
            "from\t2013\t0.79598\n",
            "kabul\t21073\t0.00447\n",
            ".\t1012\t0.99412\n",
            "today\t2651\t0.01406\n",
            ",\t1010\t0.99951\n",
            "the\t1996\t0.73782\n",
            "house\t2160\t0.00046\n",
            "took\t2165\t0.00753\n",
            "a\t1037\t0.99272\n",
            "significant\t3278\t0.12915\n",
            "step\t3357\t0.88248\n",
            "toward\t2646\t0.52469\n",
            "making\t2437\t0.23701\n",
            "historic\t3181\t0.00014\n",
            "investments\t10518\t0.00296\n",
            "that\t2008\t0.95945\n",
            "will\t2097\t0.06223\n",
            "transform\t10938\t0.14817\n",
            "america\t2637\t0.05471\n",
            "and\t1998\t0.97845\n",
            "cut\t3013\t0.02106\n",
            "taxes\t7773\t0.02628\n",
            "for\t2005\t0.24222\n",
            "working\t2551\t0.00945\n",
            "families\t2945\t0.19936\n",
            ".\t1012\t0.99385\n",
            "i\t1045\t0.59098\n",
            "want\t2215\t0.77928\n",
            "to\t2000\t0.99992\n",
            "thank\t4067\t0.48011\n",
            "everyone\t3071\t0.10245\n",
            "who\t2040\t0.96163\n",
            "voted\t5444\t0.10682\n",
            "today\t2651\t0.00253\n",
            "in\t1999\t0.98415\n",
            "support\t2490\t0.20839\n",
            "of\t1997\t0.99645\n",
            "the\t1996\t0.99651\n",
            "john\t2198\t0.02036\n",
            "lewis\t4572\t0.02457\n",
            "voting\t6830\t0.04409\n",
            "rights\t2916\t0.94441\n",
            "advancement\t12607\t0.00046\n",
            "act\t2552\t0.35905\n",
            ".\t1012\t0.99398\n",
            "we\t2057\t0.00831\n",
            "have\t2031\t0.94764\n",
            "helped\t3271\t0.00276\n",
            "to\t2000\t0.88231\n",
            "evacuate\t22811\t0.03545\n",
            "70\t3963\t0.00140\n",
            ",\t1010\t0.74856\n",
            "700\t6352\t0.00044\n",
            "people\t2111\t0.67730\n",
            "since\t2144\t0.65430\n",
            "august\t2257\t0.07700\n",
            "14\t2403\t0.00960\n",
            ".\t1012\t0.01143\n",
            "rt\t19387\t0.00001\n",
            "@\t1030\t0.00002\n",
            "white\t2317\t0.11266\n",
            "##house\t4580\t0.19461\n",
            ":\t1024\t0.63828\n",
            "since\t2144\t0.27398\n",
            "august\t2257\t0.11593\n",
            "14\t2403\t0.00650\n",
            ",\t1010\t0.99868\n",
            "the\t1996\t0.99726\n",
            "u\t1057\t0.99997\n",
            ".\t1012\t0.99999\n",
            "s\t1055\t0.98839\n",
            ".\t1012\t0.99742\n",
            "has\t2038\t0.55318\n",
            "evacuated\t13377\t0.00469\n",
            "and\t1998\t0.93732\n",
            "facilitated\t19601\t0.01702\n",
            "the\t1996\t0.99573\n",
            "evacuation\t13982\t0.92608\n",
            "of\t1997\t0.99960\n",
            "approximately\t3155\t0.24771\n",
            "58\t5388\t0.00062\n",
            ",\t1010\t0.99777\n",
            "700\t6352\t0.00348\n",
            "people\t2111\t0.58792\n",
            ".\t1012\t0.99877\n",
            "Genre: Wikipedia , mean perplexity: 1.0190401772421787\n",
            "Genre: Yelp , mean perplexity: 1.1378479186486066\n",
            "Genre: Fiction , mean perplexity: 1.593446735324658\n",
            "Genre: Twitter , mean perplexity: 1.0554655182504862\n"
          ]
        }
      ],
      "source": [
        "# running this might take up to 10 minutes\n",
        "perplexity_by_genre = calculate_perplexity_by_genre(text_by_genre)\n",
        "for genre in perplexity_by_genre:\n",
        "  print(\"Genre:\",genre,\", mean perplexity:\",np.mean(perplexity_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWk0jHUM8UFP"
      },
      "source": [
        "##Question: \n",
        "What do you think are the reasons for the wide variation in perplexity of different categories of corpus? (hint: think about the training data of the pre-trained BERT model)\n",
        "\n",
        "\n",
        "Which of these is a true language model, and why?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0S4W4bkXywIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de281ac602534a7d8d8e78ebe4c6a5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c9d6528cb304849b1fe7fba339a46f4",
              "IPY_MODEL_3972be8a14d64e7bbff0c5b9595cb952",
              "IPY_MODEL_53c807c61e5d4e40bfe7074ece30b075"
            ],
            "layout": "IPY_MODEL_46a91a8363ec4368b2191b2869f991b9"
          }
        },
        "7c9d6528cb304849b1fe7fba339a46f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58daf567c2e148c7ae514ed58615adff",
            "placeholder": "​",
            "style": "IPY_MODEL_01a29770035642cf9c9d3ddeb3220f32",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3972be8a14d64e7bbff0c5b9595cb952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4f326ba37f4a47a121bcfb4dd86bb8",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8131b083583d44d09f3d37001e8e50ec",
            "value": 570
          }
        },
        "53c807c61e5d4e40bfe7074ece30b075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e8dce90c1e4b088fb82bea510ca30b",
            "placeholder": "​",
            "style": "IPY_MODEL_60fcb3ba73454e21b1b09dd5e5054bdc",
            "value": " 570/570 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "46a91a8363ec4368b2191b2869f991b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58daf567c2e148c7ae514ed58615adff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01a29770035642cf9c9d3ddeb3220f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b4f326ba37f4a47a121bcfb4dd86bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8131b083583d44d09f3d37001e8e50ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27e8dce90c1e4b088fb82bea510ca30b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60fcb3ba73454e21b1b09dd5e5054bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c21aa4fb56948b98ac5105ca484079e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c11834c0baed4e5790f08877d93e17d1",
              "IPY_MODEL_f049886c3e2247659b44c723dd08e636",
              "IPY_MODEL_9bdc693393ec44ca93bfb40e3b9bcb35"
            ],
            "layout": "IPY_MODEL_6fb7dccba59d41d68b31aea69119b1b9"
          }
        },
        "c11834c0baed4e5790f08877d93e17d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f638136b7ff946c49c060095acadb8ea",
            "placeholder": "​",
            "style": "IPY_MODEL_d74b3a6731fc43e3962d103b15d31adb",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "f049886c3e2247659b44c723dd08e636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb77a3bcbdce4eed945d4da037f9e3ad",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f54b637ab8a40e2a0d60660e68c9e18",
            "value": 440473133
          }
        },
        "9bdc693393ec44ca93bfb40e3b9bcb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95dd77dcfdf4ccd95dc0479d2cc687d",
            "placeholder": "​",
            "style": "IPY_MODEL_90d62e7c1b8c418d823bb2646768d1c1",
            "value": " 440M/440M [00:04&lt;00:00, 98.1MB/s]"
          }
        },
        "6fb7dccba59d41d68b31aea69119b1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f638136b7ff946c49c060095acadb8ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d74b3a6731fc43e3962d103b15d31adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb77a3bcbdce4eed945d4da037f9e3ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f54b637ab8a40e2a0d60660e68c9e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e95dd77dcfdf4ccd95dc0479d2cc687d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d62e7c1b8c418d823bb2646768d1c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81f030b4c22242018470f71a09734a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e04d66555054d0fa0f102577855b06b",
              "IPY_MODEL_2b2a2e1827264ce5bd3e7d8a1e545d29",
              "IPY_MODEL_1633581bcd594a65be41bc08d1500f47"
            ],
            "layout": "IPY_MODEL_0b080e5c16f64116861265c3a0f1407c"
          }
        },
        "0e04d66555054d0fa0f102577855b06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80dd42969963458bbfe608649994bd17",
            "placeholder": "​",
            "style": "IPY_MODEL_f61f7c3891f445d49e15f948886552b5",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "2b2a2e1827264ce5bd3e7d8a1e545d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a61d2eb71646dc8156da81c156d44f",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1fa545a92214bd99397551a285a55ee",
            "value": 28
          }
        },
        "1633581bcd594a65be41bc08d1500f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1bfce518d34e71b0592ece1f4c01a3",
            "placeholder": "​",
            "style": "IPY_MODEL_8606f9022bc249438c636d85aeca946f",
            "value": " 28.0/28.0 [00:00&lt;00:00, 790B/s]"
          }
        },
        "0b080e5c16f64116861265c3a0f1407c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80dd42969963458bbfe608649994bd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61f7c3891f445d49e15f948886552b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a61d2eb71646dc8156da81c156d44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1fa545a92214bd99397551a285a55ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f1bfce518d34e71b0592ece1f4c01a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8606f9022bc249438c636d85aeca946f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cb0c3cd143343c4a0cdca0a52bcebd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_374b0557ce2544109f837cfbd677bd87",
              "IPY_MODEL_e67fd60d2b214b2088bc40629a17fef8",
              "IPY_MODEL_654399629b4a42d9bbeea3bdf55f524b"
            ],
            "layout": "IPY_MODEL_f482e067586f49cc96827b2d2a5e3004"
          }
        },
        "374b0557ce2544109f837cfbd677bd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_492871578c3c4e9e909e2fdbeee6cb9f",
            "placeholder": "​",
            "style": "IPY_MODEL_7f37dd70377141ee8d6786543f4facab",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "e67fd60d2b214b2088bc40629a17fef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e10cf5a9b84a619e565fc730447405",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eeb10c1c5d3b49c8ad1c450f14aacce4",
            "value": 231508
          }
        },
        "654399629b4a42d9bbeea3bdf55f524b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7e65052db74752a39c50dbc7d9ec09",
            "placeholder": "​",
            "style": "IPY_MODEL_8f81a5e495754a03ba3d2d4b99e3752b",
            "value": " 232k/232k [00:00&lt;00:00, 2.87MB/s]"
          }
        },
        "f482e067586f49cc96827b2d2a5e3004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492871578c3c4e9e909e2fdbeee6cb9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f37dd70377141ee8d6786543f4facab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e10cf5a9b84a619e565fc730447405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb10c1c5d3b49c8ad1c450f14aacce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c7e65052db74752a39c50dbc7d9ec09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f81a5e495754a03ba3d2d4b99e3752b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99c98c6daef44a769fadfd31efe60212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c6135490cbb4a5cb5bbe56d2210a759",
              "IPY_MODEL_3b8d547835ac4a8780a78b39aea1ee89",
              "IPY_MODEL_e642ea1dfae24d80a4ece7c4b98d0701"
            ],
            "layout": "IPY_MODEL_9f932a0e10ae4f0baecd1f95c04d5988"
          }
        },
        "4c6135490cbb4a5cb5bbe56d2210a759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb33d261b427495bbcae82fed3c5506b",
            "placeholder": "​",
            "style": "IPY_MODEL_4e464fce8e6948b7968a63c743c4e185",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "3b8d547835ac4a8780a78b39aea1ee89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d32634548148baba5b6bf91ba04217",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_062298e60b3b43ef8edd2e978a03d7e1",
            "value": 466062
          }
        },
        "e642ea1dfae24d80a4ece7c4b98d0701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68c5f0ba9f994e15817feeee55eb7e69",
            "placeholder": "​",
            "style": "IPY_MODEL_44fb6c7c03f64e3cac23d4668adc21f1",
            "value": " 466k/466k [00:00&lt;00:00, 4.59MB/s]"
          }
        },
        "9f932a0e10ae4f0baecd1f95c04d5988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb33d261b427495bbcae82fed3c5506b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e464fce8e6948b7968a63c743c4e185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d32634548148baba5b6bf91ba04217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062298e60b3b43ef8edd2e978a03d7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68c5f0ba9f994e15817feeee55eb7e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44fb6c7c03f64e3cac23d4668adc21f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}